{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear all variables\n",
        "%reset -f\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**--- Part I ---**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**0. System check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check system: \n",
        "import sys\n",
        "print(sys.executable)\n",
        "!where python          # Windows shell command, works with '!' in notebooks\n",
        "!python --version      # Check Python version\n",
        "!pip list              # Check installed packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# --- Set project root directory automatically ---\n",
        "project_root = os.path.abspath(\".\")\n",
        "os.chdir(project_root)\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "# OPTIONAL: Uncomment and customize this if running locally on  own machine\n",
        "# os.chdir(r\"C:\\Users\\YourName\\Documents\\Data and code\")\n",
        "# print(\"Manually set Working Directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dikncsIXq7pI"
      },
      "source": [
        "**1. Cleaning data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPDm0Eb1qyDG"
      },
      "outputs": [],
      "source": [
        "# ✅ Running locally in VS Code (skip this cell if using Google Colab)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ Step 1: Define the path to the local CSV file\n",
        "file_path = \"Document_Data_Download-2025-06-16.csv\"\n",
        "\n",
        "# ✅ Step 2: Load the dataset using recommended settings\n",
        "data = pd.read_csv(\n",
        "    file_path,\n",
        "    encoding=\"ISO-8859-1\",   # Matches the file’s encoding (use 'utf-8' if this fails)\n",
        "    quotechar='\"',           # Properly parses quoted fields with commas\n",
        "    low_memory=False         # Reads entire file to infer data types correctly\n",
        ")\n",
        "\n",
        "# ✅ Step 3: Sanity checks after import\n",
        "print(\"✅ Data loaded successfully\")\n",
        "print(\"Shape:\", data.shape)                  # (rows, columns)\n",
        "print(\"Columns:\", data.columns.tolist())     # List of column names\n",
        "data.head()                                  # Display the first few rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "collapsed": true,
        "id": "wXsyoBUdtYun",
        "outputId": "967964fe-2ec8-4a8d-d932-768a8d5ee4e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ Step 3: Preview data\n",
        "print(\"✅ Dataset loaded successfully!\")\n",
        "print(f\"Shape: {data.shape}\") # see number of rows and number of variables\n",
        "display(data.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ✅ Generate column summary\n",
        "column_summary = data.describe(include='all').transpose()\n",
        "column_summary[\"null_count\"] = data.isnull().sum()\n",
        "column_summary[\"data_type\"] = data.dtypes\n",
        "\n",
        "# ✅ Display summary table\n",
        "column_summary = column_summary.reset_index().rename(columns={\"index\": \"Variable\"})\n",
        "column_summary.head(20)  # You can adjust this number to see more\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "collapsed": true,
        "id": "T7rb15rR1ZZG",
        "outputId": "3688191e-842e-4420-e470-4b142f5fa09f"
      },
      "outputs": [],
      "source": [
        "# Create a fullinfo-style missing data summary\n",
        "missing_summary = pd.DataFrame({\n",
        "    \"Variable\": data.columns,\n",
        "    \"Missing Count\": data.isnull().sum().values,\n",
        "    \"Non-Missing Count\": data.notnull().sum().values,\n",
        "    \"Missing %\": (data.isnull().mean() * 100).round(2)\n",
        "})\n",
        "\n",
        "# Sort by most missing\n",
        "missing_summary = missing_summary.sort_values(by=\"Missing %\", ascending=False)\n",
        "\n",
        "# Display the result\n",
        "missing_summary\n",
        "# missing_summary.head(20)  # Show top 20\n",
        "\n",
        "#  Key Reasons for Missing Data in Specific Columns\n",
        "# Variable\tWhy It's Often Missing\n",
        "# Hazard (95%)\tOnly tagged when a document explicitly addresses climate hazards (e.g., floods, droughts). Most policies are not disaster-risk-focused.\n",
        "# Framework (94%)\tOnly applies to overarching “framework” laws. Most documents are not categorized as frameworks (see definition in CCLW methodology).\n",
        "# Collection Description(s) & Title(s)\tThese apply only to documents grouped in special CPR/CCLW collections (e.g., EU climate directives). Most policies are not part of such curated sets.\n",
        "# Author & Author Type (70%)\tOften unavailable in source PDFs or not yet extracted. Metadata enrichment is ongoing.\n",
        "# Document Variant (63%)\tPopulated only for translations, amendments, or alternative versions — not for original laws.\n",
        "# Instrument (47%)\tTagged only when a document clearly specifies policy tools (e.g., regulation, investment). Many documents are unclassified here.\n",
        "# Keyword (44%)\tOptional manual tags — not exhaustive. Many records don't include them.\n",
        "# Sector (36%)\tPolicies that are cross-sectoral or general may lack specific sector tags.\n",
        "# Topic/Response (31%)\tThese depend on manual classification into mitigation, adaptation, etc. Older or unprocessed records may be missing this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "collapsed": true,
        "id": "54U3-hwt09Hs",
        "outputId": "ad8f79a2-6ac7-458d-c746-7550dd1cb515"
      },
      "outputs": [],
      "source": [
        "# Check for exact duplicate rows\n",
        "duplicate_count = data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "data[data.duplicated()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfFhOBBT0-VG"
      },
      "outputs": [],
      "source": [
        "print(\"Sector Distribution:\")\n",
        "print(data[\"Sector\"].value_counts(dropna=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5W7ukiF1F4j"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTopic/Response Distribution:\")\n",
        "print(data[\"Topic/Response\"].value_counts(dropna=False))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Dealing with time and event**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2.1. --- --- Key modification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 1: safe_parse()\n",
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    # Safely parse dates\n",
        "    paired = [(typ, safe_parse(d)) for typ, d in zip(type_list, date_list)]\n",
        "    \n",
        "    # Remove any with unparseable dates\n",
        "    paired = [x for x in paired if x[1] is not None]\n",
        "    \n",
        "    # Sort by date\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "# Apply to DataFrame\n",
        "data[\"event_timeline\"] = data.apply(extract_sorted_events, axis=1)\n",
        "\n",
        "# Example: View first result\n",
        "data[\"event_timeline\"].dropna().iloc[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "# Apply it like this\n",
        "data = order_after(data, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create additional variables based on the cleaned and sorted event_timeline\n",
        "\n",
        "# 1. Number of amendments\n",
        "data[\"num_amendments\"] = data[\"event_timeline\"].apply(\n",
        "    lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        ")\n",
        "\n",
        "# 2. Date when the law was first passed/approved\n",
        "data[\"first_passed_date\"] = data[\"event_timeline\"].apply(\n",
        "    lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        ")\n",
        "\n",
        "# 3. Date of most recent event (e.g., latest amendment or update)\n",
        "data[\"last_event_date\"] = data[\"event_timeline\"].apply(\n",
        "    lambda x: x[-1][1] if x else None\n",
        ")\n",
        "\n",
        "# 4. Policy duration in years (from first to last event)\n",
        "from numpy import nan\n",
        "data[\"policy_duration_years\"] = data.apply(\n",
        "    lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "    if row[\"last_event_date\"] and row[\"first_passed_date\"] else nan,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Optional: Preview the new columns\n",
        "data[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Order the extra variable after event\n",
        "def order_multiple_after(df, new_cols, after_col):\n",
        "    cols = list(df.columns)\n",
        "    for col in new_cols:\n",
        "        if col in cols:\n",
        "            cols.remove(col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    for col in reversed(new_cols):  # insert in reverse to maintain order\n",
        "        cols.insert(insert_at, col)\n",
        "    return df[cols]\n",
        "\n",
        "# Columns you created\n",
        "new_cols = [\"num_amendments\", \"first_passed_date\", \"last_event_date\", \"policy_duration_years\"]\n",
        "\n",
        "# Reorder them after 'event_timeline'\n",
        "data = order_multiple_after(data, new_cols, \"event_timeline\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2.2. --- --- Important filter - Full timeline of events (types)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count before filtering\n",
        "original_count = data.shape[0]\n",
        "\n",
        "# Apply filter\n",
        "exclude_keywords = [\"Other\", \"Repealed\", \"Superseded\"]\n",
        "data = data[~data[\"Full timeline of events (types)\"]\n",
        "            .fillna(\"\")\n",
        "            .str.contains('|'.join(exclude_keywords), case=False)]\n",
        "\n",
        "# Reset index\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# Count after filtering\n",
        "remaining_count = data.shape[0]\n",
        "removed_count = original_count - remaining_count\n",
        "\n",
        "# Print results\n",
        "print(f\"Rows removed: {removed_count}\")\n",
        "print(f\"Rows remaining: {remaining_count}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Cleaning & Paste**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Step 1: Define a cleaning function\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"<.*?>\", \" \", str(text))  # Remove HTML\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
        "    return text\n",
        "\n",
        "# Step 2: Apply to Family Summary (fallback to Document Title if missing)\n",
        "data[\"text_for_indicator\"] = data[\"Family Summary\"].fillna(data[\"Document Title\"])\n",
        "data[\"text_clean\"] = data[\"text_for_indicator\"].apply(clean_text)\n",
        "\n",
        "# Show preview of cleaned text\n",
        "data[[\"text_for_indicator\", \"text_clean\"]].head(5)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**--- Part II ---**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [4]. [Ambition] with un-supervise methods\n",
        "\n",
        "Justification for Ambition Indicator Method\n",
        "\n",
        "Because we do not have labeled training data for \"high-ambition\" vs. \"low-ambition\" policies, and the policy documents follow a structured format, we use a rule-based keyword approach. This method allows us to transparently score each policy based on clearly defined terms (e.g., net zero, just transition), which can be easily replicated or refined. Our approach is inspired by methods used in recent studies such as the Climate Policy Radar's CPR indicators.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1. --- --- keywords scoring + weighting"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Main codes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Define expanded ambition keyword list (called version 3)\n",
        "# dictionary from Net zero tracker\n",
        "ambition_keywords = [\n",
        "    \"net zero\", \"carbon neutrality\", \"zero emissions\", \"carbon neutral\", \n",
        "    \"climate neutrality\", \"green transition\", \"energy transition\",\n",
        "    \"long-term strategy\", \"roadmap\", \"vision\", \"trajectory\", \"future-proof\", \n",
        "    \"system transformation\", \"transformational change\",\n",
        "    \"reduce emissions\", \"emissions reduction\", \"reduction target\", \n",
        "    \"national target\", \"carbon pricing\", \"emissions trading\",\n",
        "    \"goal\", \"objective\", \"target\", \"aim\", \"commit\", \"pledge\", \n",
        "    \"prioritize\", \"aspire\", \"intend\", \"will reduce\", \"intends to reduce\",\n",
        "    \"just transition\", \"equity\", \"inclusive\", \"social justice\",\n",
        "    \"100% renewable\", \"clean energy\", \"phase out coal\", \"climate-resilient\", \n",
        "    \"green economy\", \"resilience building\", \"adaptive capacity\", \n",
        "    \"paris agreement\", \"ndc\", \"cop26\", \"cop27\", \"global climate action\",\n",
        "    \"zero carbon\", \"carbon negative\", \"net negative\", \"climate positive\",\n",
        "    \"science-based target\", \"1.5°c target\", \"1.5 degree\", \"baseline year\", \"emissions baseline\",\n",
        "    \"absolute emissions\", \"emissions intensity\", \"business as usual\", \"bau\",\n",
        "    \"reporting mechanism\", \"accountability\", \"review mechanism\", \"measures and steps\", \n",
        "    \"implementation plan\", \"executive accountability\", \"executive pay linkage\",\n",
        "    \"carbon credit\", \"offsets\", \"removals target\", \"carbon removal\", \n",
        "    \"nature-based solutions\", \"ccs\", \"beccs\", \"dac\", \"cdr\",\n",
        "    \"scope 1\", \"scope 2\", \"scope 3\", \n",
        "    \"territorial emissions\", \"consumption emissions\", \n",
        "    \"international aviation\", \"international shipping\"\n",
        "]\n",
        "\n",
        "# Step 2: Count number of matching ambition keywords per Family Summary\n",
        "def count_ambition_keywords(text):\n",
        "    if pd.isna(text):\n",
        "        return 0\n",
        "    text = str(text).lower()\n",
        "    return sum(1 for word in ambition_keywords if word in text)\n",
        "\n",
        "data[\"ambition_score_raw\"] = data[\"Family Summary\"].apply(count_ambition_keywords)\n",
        "\n",
        "# Step 3: Define ambition weights by Document Type\n",
        "ambition_weights = {\n",
        "    \"Vision\": 1.5,\n",
        "    \"Long-Term Strategy\": 1.5,\n",
        "    \"Roadmap\": 1.5,\n",
        "    \"Strategy\": 1.0,\n",
        "    \"Plan\": 1.0,\n",
        "    \"Action Plan\": 1.0,\n",
        "    \"Policy\": 1.0,\n",
        "    \"Law\": 0.9,\n",
        "    \"Act\": 0.9,\n",
        "    \"Regulation\": 0.9,\n",
        "    \"Press Release\": 0.7,\n",
        "    \"Report\": 0.7,\n",
        "    \"Progress Report\": 0.7,\n",
        "    \"Summary\": 0.7,\n",
        "    \"Submission\": 0.5,\n",
        "    \"Communication\": 0.5,\n",
        "    \"National Communication\": 0.5\n",
        "}\n",
        "\n",
        "def get_ambition_weight(doc_type):\n",
        "    if pd.isna(doc_type):\n",
        "        return 1.0\n",
        "    for key in ambition_weights:\n",
        "        if key.lower() in doc_type.lower():\n",
        "            return ambition_weights[key]\n",
        "    return 1.0  # fallback/default\n",
        "\n",
        "data[\"ambition_weight\"] = data[\"Document Type\"].apply(get_ambition_weight)\n",
        "\n",
        "# Step 4: Compute final adjusted score\n",
        "data[\"ambition_score_adjusted\"] = data[\"ambition_score_raw\"] * data[\"ambition_weight\"]\n",
        "\n",
        "# Step 5: Reorder ambition columns after policy_duration_years\n",
        "ambition_cols = [\"ambition_score_raw\", \"ambition_weight\", \"ambition_score_adjusted\"]\n",
        "data = order_multiple_after(data, ambition_cols, \"policy_duration_years\")\n",
        "\n",
        "# Step 6: Preview results\n",
        "data[ambition_cols + [\"Document Type\", \"Family Summary\"]].head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Define expanded ambition keyword list (Key ver 3) - with normalization\n",
        "# dictionary from Net zero tracker\n",
        "ambition_keywords = [\n",
        "    \"net zero\", \"carbon neutrality\", \"zero emissions\", \"carbon neutral\", \n",
        "    \"climate neutrality\", \"green transition\", \"energy transition\",\n",
        "    \"long-term strategy\", \"roadmap\", \"vision\", \"trajectory\", \"future-proof\", \n",
        "    \"system transformation\", \"transformational change\",\n",
        "    \"reduce emissions\", \"emissions reduction\", \"reduction target\", \n",
        "    \"national target\", \"carbon pricing\", \"emissions trading\",\n",
        "    \"goal\", \"objective\", \"target\", \"aim\", \"commit\", \"pledge\", \n",
        "    \"prioritize\", \"aspire\", \"intend\", \"will reduce\", \"intends to reduce\",\n",
        "    \"just transition\", \"equity\", \"inclusive\", \"social justice\",\n",
        "    \"100% renewable\", \"clean energy\", \"phase out coal\", \"climate-resilient\", \n",
        "    \"green economy\", \"resilience building\", \"adaptive capacity\", \n",
        "    \"paris agreement\", \"ndc\", \"cop26\", \"cop27\", \"global climate action\",\n",
        "    \"zero carbon\", \"carbon negative\", \"net negative\", \"climate positive\",\n",
        "    \"science-based target\", \"1.5°c target\", \"1.5 degree\", \"baseline year\", \"emissions baseline\",\n",
        "    \"absolute emissions\", \"emissions intensity\", \"business as usual\", \"bau\",\n",
        "    \"reporting mechanism\", \"accountability\", \"review mechanism\", \"measures and steps\", \n",
        "    \"implementation plan\", \"executive accountability\", \"executive pay linkage\",\n",
        "    \"carbon credit\", \"offsets\", \"removals target\", \"carbon removal\", \n",
        "    \"nature-based solutions\", \"ccs\", \"beccs\", \"dac\", \"cdr\",\n",
        "    \"scope 1\", \"scope 2\", \"scope 3\", \n",
        "    \"territorial emissions\", \"consumption emissions\", \n",
        "    \"international aviation\", \"international shipping\"\n",
        "]\n",
        "\n",
        "# Step 2: Count number of matching ambition keywords per Family Summary\n",
        "def count_ambition_keywords(text):\n",
        "    if pd.isna(text):\n",
        "        return 0\n",
        "    text = str(text).lower()\n",
        "    return sum(1 for word in ambition_keywords if word in text)\n",
        "\n",
        "data[\"ambition_score_raw\"] = data[\"Family Summary\"].apply(count_ambition_keywords)\n",
        "\n",
        "# Step 3: Define ambition weights by Document Type\n",
        "ambition_weights = {\n",
        "    \"Vision\": 1.5,\n",
        "    \"Long-Term Strategy\": 1.5,\n",
        "    \"Roadmap\": 1.5,\n",
        "    \"Strategy\": 1.0,\n",
        "    \"Plan\": 1.0,\n",
        "    \"Action Plan\": 1.0,\n",
        "    \"Policy\": 1.0,\n",
        "    \"Law\": 0.9,\n",
        "    \"Act\": 0.9,\n",
        "    \"Regulation\": 0.9,\n",
        "    \"Press Release\": 0.7,\n",
        "    \"Report\": 0.7,\n",
        "    \"Progress Report\": 0.7,\n",
        "    \"Summary\": 0.7,\n",
        "    \"Submission\": 0.5,\n",
        "    \"Communication\": 0.5,\n",
        "    \"National Communication\": 0.5\n",
        "}\n",
        "\n",
        "def get_ambition_weight(doc_type):\n",
        "    if pd.isna(doc_type):\n",
        "        return 1.0\n",
        "    for key in ambition_weights:\n",
        "        if key.lower() in str(doc_type).lower():\n",
        "            return ambition_weights[key]\n",
        "    return 1.0\n",
        "\n",
        "data[\"ambition_weight\"] = data[\"Document Type\"].apply(get_ambition_weight)\n",
        "\n",
        "# Step 4: Compute final adjusted score\n",
        "data[\"ambition_score_adjusted\"] = data[\"ambition_score_raw\"] * data[\"ambition_weight\"]\n",
        "\n",
        "# Step 5: Normalize by Family Summary length\n",
        "data[\"family_summary_length\"] = data[\"Family Summary\"].astype(str).str.split().str.len()\n",
        "data[\"family_summary_length\"] = data[\"family_summary_length\"].replace(0, pd.NA)\n",
        "data[\"ambition_score_normalized\"] = data[\"ambition_score_adjusted\"] / data[\"family_summary_length\"]\n",
        "\n",
        "# Step 6: Reorder ambition columns after policy_duration_years\n",
        "ambition_cols = [\n",
        "    \"ambition_score_raw\", \"ambition_weight\", \"ambition_score_adjusted\",\n",
        "    \"family_summary_length\", \"ambition_score_normalized\"\n",
        "]\n",
        "data = order_multiple_after(data, ambition_cols, \"policy_duration_years\")\n",
        "\n",
        "\n",
        "# Local display version\n",
        "print(data[ambition_cols + [\"Document Type\", \"Family Summary\"]].head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key version 4: Based on CPR ICLR 2024 target definitions (enhancement to Version 3)\n",
        "# CPR = Climate Policy Radar\n",
        "# ICLR 2024 — the International Conference on Learning Representations, one of the top global conferences for AI and machine learning research.\n",
        "# original work by the Climate Policy Radar team (they’re the authors)\n",
        "# CPR uses and builds upon the Climate Change Laws of the World (CCLW) database, \n",
        "# which is hosted by the Grantham Research Institute at the London School of Economics (LSE).\n",
        "# The CPR dataset includes documents from CCLW, along with additional documents from the UNFCCC and other national sources.\n",
        "#climate-laws.org pages (under document details) mention CPR for machine learning and NLP-enhanced exploration of the same policy texts.\n",
        "# The paper: IDENTIFYING CLIMATE TARGETS IN NATIONAL LAWS AND POLICIES USING MACHINE LEARNING \n",
        "# presents machine learning method to extract Net Zero, Reduction, and Other targets\n",
        "# outlines how they trained machine learning models to automatically detect climate targets\n",
        "# Sharing pretrained language models (e.g. BERT, RoBERTa).\n",
        "\n",
        "ambition_keywords_v4 = ambition_keywords + [\n",
        "    # GHG-specific\n",
        "    \"co2\", \"co2e\", \"methane\", \"ghg\", \"greenhouse gas\",\n",
        "    \n",
        "    # Sectoral\n",
        "    \"energy efficiency\", \"transport emissions\", \"agriculture\", \"forestry\",\n",
        "    \"deforestation\", \"reforestation\", \"land use\", \"waste\", \"water use\", \"industrial emissions\",\n",
        "    \n",
        "    # Quantified action words\n",
        "    \"halve\", \"double\", \"increase by\", \"reduce by\", \"cut by\", \"scale up\", \"achieve\",\n",
        "    \n",
        "    # Temporal phrases (best used with regex later)\n",
        "    \"by 2030\", \"by 2050\", \"in 5 years\", \"in the next decade\"\n",
        "]\n",
        "ambition_keywords_v4 = list(set(ambition_keywords_v4))  # deduplicate\n",
        "\n",
        "def count_keywords_v4(text):\n",
        "    if pd.isna(text):\n",
        "        return 0\n",
        "    text = str(text).lower()\n",
        "    return sum(1 for word in ambition_keywords_v4 if word in text)\n",
        "\n",
        "# Create additional columns\n",
        "data[\"ambition_score_raw_v4\"] = data[\"Family Summary\"].apply(count_keywords_v4)\n",
        "data[\"ambition_score_adjusted_v4\"] = data[\"ambition_score_raw_v4\"] * data[\"ambition_weight\"]\n",
        "data[\"ambition_score_normalized_v4\"] = data[\"ambition_score_adjusted_v4\"] / data[\"family_summary_length\"]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Aggregate to country-year data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Extract year from \"First event in timeline\" safely\n",
        "data[\"document_year\"] = pd.to_datetime(data[\"First event in timeline\"], errors='coerce').dt.year\n",
        "\n",
        "# Step 2: Drop rows with missing or invalid year or country\n",
        "data = data.dropna(subset=[\"document_year\", \"Geographies\"])\n",
        "data[\"document_year\"] = data[\"document_year\"].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Assign document_year \n",
        "def extract_year(row):\n",
        "    if pd.notna(row[\"first_passed_date\"]):\n",
        "        return pd.to_datetime(row[\"first_passed_date\"]).year\n",
        "    elif pd.notna(row[\"last_event_date\"]):\n",
        "        return pd.to_datetime(row[\"last_event_date\"]).year\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "data[\"document_year\"] = data.apply(extract_year, axis=1)\n",
        "data = data.dropna(subset=[\"document_year\", \"Geographies\"])\n",
        "data[\"document_year\"] = data[\"document_year\"].astype(int)\n",
        "\n",
        "# Step 2: Add binary indicator for high-weight documents (weight ≥ 1.0)\n",
        "data[\"is_high_weight\"] = (data[\"ambition_weight\"] >= 1.0).astype(int)\n",
        "\n",
        "# Step 3: Aggregate by country-year\n",
        "ambition_panel = (\n",
        "    data\n",
        "    .groupby([\"Geographies\", \"document_year\"])\n",
        "    .agg(\n",
        "        num_docs=(\"Family Summary\", \"count\"),\n",
        "        mean_raw=(\"ambition_score_raw\", \"mean\"),\n",
        "        mean_weight=(\"ambition_weight\", \"mean\"),\n",
        "        share_high_weight_docs=(\"is_high_weight\", \"mean\"),  # mean of 0/1 = proportion\n",
        "        mean_adjusted=(\"ambition_score_adjusted\", \"mean\"),\n",
        "        total_adjusted=(\"ambition_score_adjusted\", \"sum\")\n",
        "    )\n",
        "    .reset_index()\n",
        "    .sort_values([\"Geographies\", \"document_year\"])\n",
        ")\n",
        "\n",
        "# Optional: round for clarity\n",
        "ambition_panel[[\"mean_raw\", \"mean_weight\", \"share_high_weight_docs\", \"mean_adjusted\"]] = \\\n",
        "    ambition_panel[[\"mean_raw\", \"mean_weight\", \"share_high_weight_docs\", \"mean_adjusted\"]].round(3)\n",
        "\n",
        "# Preview\n",
        "ambition_panel.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Add ISO3 code from original data ===\n",
        "iso_map = data[[\"Geographies\", \"Geography ISOs\"]].dropna().drop_duplicates()\n",
        "ambition_panel = ambition_panel.merge(iso_map, on=\"Geographies\", how=\"left\")\n",
        "\n",
        "# === (b) Reorder columns and rename for clarity ===\n",
        "ambition_panel.rename(columns={\n",
        "    \"Geography ISOs\": \"iso3\",\n",
        "    \"document_year\": \"year\"\n",
        "}, inplace=True)\n",
        "\n",
        "# === (c) Move iso3 and year to front\n",
        "cols = [\"iso3\", \"year\"] + [col for col in ambition_panel.columns if col not in [\"iso3\", \"year\", \"Geographies\"]]\n",
        "ambition_panel = ambition_panel[cols]\n",
        "\n",
        "# === (d) Save to CSV\n",
        "ambition_panel.to_csv(\"ambition_keywordscore.csv\", index=False)\n",
        "print(\"✅ Saved to: ambition_keywordscore.csv\")\n",
        "\n",
        "# === (e) Preview\n",
        "ambition_panel.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full Aggregation Code for v4 Ambition Variables (update version of version 3)\n",
        "\n",
        "# === (a) Reuse year and ISO preparation ===\n",
        "def extract_year(row):\n",
        "    if pd.notna(row[\"first_passed_date\"]):\n",
        "        return pd.to_datetime(row[\"first_passed_date\"]).year\n",
        "    elif pd.notna(row[\"last_event_date\"]):\n",
        "        return pd.to_datetime(row[\"last_event_date\"]).year\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "data[\"document_year\"] = data.apply(extract_year, axis=1)\n",
        "data = data.dropna(subset=[\"document_year\", \"Geographies\"])\n",
        "data[\"document_year\"] = data[\"document_year\"].astype(int)\n",
        "data[\"is_high_weight\"] = (data[\"ambition_weight\"] >= 1.0).astype(int)\n",
        "\n",
        "# === (b) Group and aggregate v4 indicators ===\n",
        "ambition_panel_v4 = (\n",
        "    data\n",
        "    .groupby([\"Geographies\", \"document_year\"])\n",
        "    .agg(\n",
        "        num_docs=(\"Family Summary\", \"count\"),\n",
        "        mean_raw_v4=(\"ambition_score_raw_v4\", \"mean\"),\n",
        "        mean_weight=(\"ambition_weight\", \"mean\"),\n",
        "        share_high_weight_docs=(\"is_high_weight\", \"mean\"),\n",
        "        mean_adjusted_v4=(\"ambition_score_adjusted_v4\", \"mean\"),\n",
        "        total_adjusted_v4=(\"ambition_score_adjusted_v4\", \"sum\"),\n",
        "        mean_normalized_v4=(\"ambition_score_normalized_v4\", \"mean\")\n",
        "    )\n",
        "    .reset_index()\n",
        "    .sort_values([\"Geographies\", \"document_year\"])\n",
        ")\n",
        "\n",
        "# === (c) Add ISO codes from the full data\n",
        "iso_map = data[[\"Geographies\", \"Geography ISOs\"]].dropna().drop_duplicates()\n",
        "ambition_panel_v4 = ambition_panel_v4.merge(iso_map, on=\"Geographies\", how=\"left\")\n",
        "\n",
        "# === (d) Rename and reorder columns\n",
        "ambition_panel_v4.rename(columns={\n",
        "    \"Geography ISOs\": \"iso3\",\n",
        "    \"document_year\": \"year\"\n",
        "}, inplace=True)\n",
        "\n",
        "cols_ordered = [\"iso3\", \"year\"] + [col for col in ambition_panel_v4.columns if col not in [\"iso3\", \"year\", \"Geographies\"]]\n",
        "ambition_panel_v4 = ambition_panel_v4[cols_ordered]\n",
        "\n",
        "# === (e) Round numerical values for presentation\n",
        "cols_to_round = [col for col in ambition_panel_v4.columns if col.startswith(\"mean_\") or col.startswith(\"total_\") or col.startswith(\"share_\")]\n",
        "ambition_panel_v4[cols_to_round] = ambition_panel_v4[cols_to_round].round(3)\n",
        "\n",
        "# === (f) Export to CSV\n",
        "ambition_panel_v4.to_csv(\"ambition_keywordscore_enhance.csv\", index=False)\n",
        "print(\"✅ Saved to: ambition_keywordscore_enhance.csv\")\n",
        "\n",
        "# Preview\n",
        "ambition_panel_v4.head(10)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Statistics, Trends**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistic (not aggregate)\n",
        "import pandas as pd\n",
        "\n",
        "# List of ambition-related variables\n",
        "ambition_vars = [\"ambition_score_raw\", \"ambition_weight\", \"ambition_score_adjusted\"]\n",
        "\n",
        "# Separate numeric and categorical\n",
        "numeric_vars = [col for col in ambition_vars if pd.api.types.is_numeric_dtype(data[col])]\n",
        "categorical_vars = [col for col in ambition_vars if col not in numeric_vars]\n",
        "\n",
        "# 1️⃣ Stata-style summary for numeric variables\n",
        "numeric_summary = data[numeric_vars].agg(['count', 'mean', 'std', 'min', 'max']).T\n",
        "numeric_summary = numeric_summary.rename(columns={\n",
        "    \"count\": \"N\",\n",
        "    \"mean\": \"Mean\",\n",
        "    \"std\": \"Std. Dev.\",\n",
        "    \"min\": \"Min\",\n",
        "    \"max\": \"Max\"\n",
        "})\n",
        "numeric_summary = numeric_summary.round(2)\n",
        "\n",
        "# Print numeric summary\n",
        "print(\"Stata-style Summary Statistics for Numeric Variables:\\n\")\n",
        "print(numeric_summary)\n",
        "\n",
        "# 2️⃣ Tabulation for categorical variables (none expected, but for completeness)\n",
        "for col in categorical_vars:\n",
        "    print(f\"\\nStata-style Tabulation for: {col}\")\n",
        "    tab = data[col].value_counts(dropna=False).reset_index()\n",
        "    tab.columns = [col, \"Frequency\"]\n",
        "    tab[\"Percent\"] = (tab[\"Frequency\"] / tab[\"Frequency\"].sum() * 100).round(2)\n",
        "    print(tab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# statistics for a specific year (aggregate country-year)\n",
        "# Filter to 2016 only\n",
        "ambition_2016 = ambition_panel[ambition_panel[\"document_year\"] == 2016]\n",
        "\n",
        "# Stata-style summary\n",
        "summary_2016 = ambition_2016[[\n",
        "    \"num_docs\", \n",
        "    \"mean_raw\", \n",
        "    \"mean_weight\", \n",
        "    \"share_high_weight_docs\", \n",
        "    \"mean_adjusted\", \n",
        "    \"total_adjusted\"\n",
        "]].agg(['count', 'mean', 'std', 'min', 'max']).T\n",
        "\n",
        "summary_2016 = summary_2016.rename(columns={\n",
        "    \"count\": \"N\", \n",
        "    \"mean\": \"Mean\", \n",
        "    \"std\": \"Std. Dev.\", \n",
        "    \"min\": \"Min\", \n",
        "    \"max\": \"Max\"\n",
        "}).round(2)\n",
        "\n",
        "print(\"Stata-style Summary Statistics for 2016:\\n\")\n",
        "print(summary_2016)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histogram of [mean_adjusted] ambition scores across countries in 2016\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(ambition_2016[\"mean_adjusted\"], bins=15, edgecolor='black')\n",
        "plt.title(\"Distribution of Mean Adjusted Ambition Scores Across Countries (2016)\")\n",
        "plt.xlabel(\"Mean Adjusted Ambition Score (per document)\")\n",
        "plt.ylabel(\"Number of Countries\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace with one of: \"num_docs\", \"mean_raw\", \"mean_weight\", \"share_high_weight_docs\"\n",
        "var_to_plot = \"num_docs\"\n",
        "\n",
        "# Histogram of selected ambition variable across countries in 2016\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(ambition_2016[var_to_plot], bins=15, edgecolor='black', color='skyblue')\n",
        "plt.title(f\"Distribution of {var_to_plot.replace('_', ' ').title()} Across Countries (2016)\")\n",
        "plt.xlabel(var_to_plot.replace('_', ' ').title())\n",
        "plt.ylabel(\"Number of Countries\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matplot across countries in 2016\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Merge ISO codes into 2016 data (if not already included)\n",
        "# Assuming ambition_panel doesn't yet include \"Geography ISOs\", merge it in:\n",
        "ambition_2016 = ambition_2016.merge(\n",
        "    data[[\"Geographies\", \"Geography ISOs\"]].drop_duplicates(),\n",
        "    on=\"Geographies\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Sort by ambition score for cleaner visualization\n",
        "ambition_2016_sorted = ambition_2016.sort_values(\"mean_adjusted\", ascending=False)\n",
        "\n",
        "# Plot bar chart\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(ambition_2016_sorted[\"Geography ISOs\"], ambition_2016_sorted[\"mean_adjusted\"], color='skyblue', edgecolor='black')\n",
        "plt.title(\"Mean Adjusted Ambition Scores Across Countries (2016)\")\n",
        "plt.xlabel(\"Country (ISO Code)\")\n",
        "plt.ylabel(\"Mean Adjusted Ambition Score (per document)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Bar Chart of 2016 Ambition Scores by ISO Code\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create some spacing between bars\n",
        "x = np.arange(len(ambition_2016_sorted))  # numeric positions for ISO codes\n",
        "width = 0.6  # narrower bars\n",
        "\n",
        "# Set figure\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Plot bar chart\n",
        "plt.bar(x, ambition_2016_sorted[\"mean_adjusted\"], width=width, color='skyblue', edgecolor='black')\n",
        "\n",
        "# X-axis with ISO codes, small font, spaced labels\n",
        "plt.xticks(ticks=x, labels=ambition_2016_sorted[\"Geography ISOs\"], rotation=90, fontsize=8)\n",
        "\n",
        "plt.title(\"Mean Adjusted Ambition Scores Across Countries (2016)\", fontsize=14)\n",
        "plt.xlabel(\"Country (ISO Code)\", fontsize=11)\n",
        "plt.ylabel(\"Mean Adjusted Ambition Score (per document)\", fontsize=11)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Line Plot of Global Ambition Trend Over Time\n",
        "# Step 1: Compute yearly global average ambition\n",
        "global_trend = (\n",
        "    ambition_panel\n",
        "    .groupby(\"document_year\")\n",
        "    .agg(global_avg_ambition=(\"mean_adjusted\", \"mean\"))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Step 2: Plot the trend\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(global_trend[\"document_year\"], global_trend[\"global_avg_ambition\"], marker='o')\n",
        "plt.title(\"Global Average Ambition Score Over Time\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Avg. Adjusted Ambition Score (per document)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create yearly global averages and totals\n",
        "global_trend = (\n",
        "    ambition_panel\n",
        "    .groupby(\"document_year\")\n",
        "    .agg(\n",
        "        mean_adjusted=(\"mean_adjusted\", \"mean\"),\n",
        "        total_adjusted=(\"total_adjusted\", \"sum\")\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Step 2: Restrict to 1990 onward\n",
        "global_trend = global_trend[global_trend[\"document_year\"] >= 1990]\n",
        "\n",
        "# Step 3: Identify the peak year in mean_adjusted\n",
        "peak_year = global_trend.loc[global_trend[\"mean_adjusted\"].idxmax(), \"document_year\"]\n",
        "peak_value = global_trend[\"mean_adjusted\"].max()\n",
        "\n",
        "# Step 4: Plot both lines\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(global_trend[\"document_year\"], global_trend[\"mean_adjusted\"], label=\"Mean Adjusted Ambition\", marker='o')\n",
        "plt.plot(global_trend[\"document_year\"], global_trend[\"total_adjusted\"], label=\"Total Adjusted Ambition\", linestyle='--', marker='x', color='orange')\n",
        "\n",
        "# Step 5: Annotate the Kyoto peak\n",
        "plt.axvline(x=peak_year, color='red', linestyle='--', label=f'Peak Year ({peak_year})')\n",
        "plt.text(peak_year + 0.5, peak_value + 0.1, f'Kyoto peak: {peak_year}', color='red')\n",
        "\n",
        "# Final touches\n",
        "plt.title(\"Global Ambition Scores Over Time (1990–Present)\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Ambition Score\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#3. xtline-style Panel Plot: Selected Countries Over Time\n",
        "# Step 1: Filter to selected countries\n",
        "selected_countries = [\"India\", \"Brazil\", \"Germany\", \"United Kingdom\", \"Kenya\"]\n",
        "filtered = ambition_panel[ambition_panel[\"Geographies\"].isin(selected_countries)]\n",
        "\n",
        "# Step 2: Plot multiple lines (like xtline in Stata)\n",
        "plt.figure(figsize=(10, 6))\n",
        "for country in selected_countries:\n",
        "    subset = filtered[filtered[\"Geographies\"] == country]\n",
        "    plt.plot(subset[\"document_year\"], subset[\"mean_adjusted\"], label=country, marker='o')\n",
        "\n",
        "plt.title(\"Mean Adjusted Ambition Score Over Time by Country\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean Adjusted Ambition Score\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top 10 and Bottom 10 Countries by Mean Ambition Score (year == 2016)\n",
        "\n",
        "# Merge ISO codes (if needed)\n",
        "ambition_2016 = ambition_panel[ambition_panel[\"document_year\"] == 2016].merge(\n",
        "    data[[\"Geographies\", \"Geography ISOs\"]].drop_duplicates(),\n",
        "    on=\"Geographies\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Top 10 countries\n",
        "top10 = ambition_2016.sort_values(\"mean_adjusted\", ascending=False).head(10)\n",
        "print(\"Top 10 Countries by Mean Ambition Score (2016):\")\n",
        "print(top10[[\"Geography ISOs\", \"Geographies\", \"mean_adjusted\"]])\n",
        "\n",
        "# Bottom 10 countries\n",
        "bottom10 = ambition_2016.sort_values(\"mean_adjusted\", ascending=True).head(10)\n",
        "print(\"\\nBottom 10 Countries by Mean Ambition Score (2016):\")\n",
        "print(bottom10[[\"Geography ISOs\", \"Geographies\", \"mean_adjusted\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Bar Plot of total_adjusted Ambition (2016)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sort by total ambition score\n",
        "ambition_2016_sorted = ambition_2016.sort_values(\"total_adjusted\", ascending=False)\n",
        "\n",
        "# Bar plot: total adjusted ambition\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.bar(ambition_2016_sorted[\"Geography ISOs\"], ambition_2016_sorted[\"total_adjusted\"], color='salmon', edgecolor='black')\n",
        "plt.title(\"Total Adjusted Ambition Scores Across Countries (2016)\")\n",
        "plt.xlabel(\"Country (ISO Code)\")\n",
        "plt.ylabel(\"Total Adjusted Ambition Score\")\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2. --- --- Topic Modeling (e.g., LDA or BERTopic)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skip (not run)\n",
        "# Install in local terminal (here is miniconda) - not need if using colab or cloud based\n",
        "conda create -n bertopic_env python=3.10 -y\n",
        "conda activate bertopic_env\n",
        "pip install bertopic[all]\n",
        "\n",
        "# Once installed, register the environment with Jupyter:\n",
        "pip install ipykernel\n",
        "python -m ipykernel install --user --name=bertopic_env --display-name \"Python (BERTopic)\"\n",
        "conda activate bertopic_env\n",
        "\n",
        "# Open Jupyter Notebook (VS Code or Jupyter Lab)\n",
        "# Click the kernel selector in the top right\n",
        "# Choose: Python (BERTopic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ipykernel\n",
        "!python -m ipykernel install --user --name=bertopic_env --display-name \"Python (BERTopic)\"\n",
        "!conda activate bertopic_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# install in bash:\n",
        "conda install -p \"c:\\Users\\AppData\\Local\\miniconda3\\envs\\bertopic_env\" ipykernel --update-deps --force-reinstall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "model = BERTopic()\n",
        "print(\"✓ BERTopic is working!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1️⃣ Import packages\n",
        "# pip install bertopic (install in terminal - miniconda)\n",
        "import pandas as pd\n",
        "import re, string\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 2️⃣ Load policy document dataset\n",
        "data = pd.read_csv(\"Document_Data_Download-2025-06-16.csv\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Standard version (fixed & reproducible)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# need to change the kernel to BERtopic\n",
        "from bertopic import BERTopic\n",
        "model = BERTopic()\n",
        "print(\"✓ BERTopic is working!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📦 Step 0: Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# 🧠 Step 1: Set up preprocessing\n",
        "download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# 2️⃣ Load policy document dataset\n",
        "data = pd.read_csv(\"Document_Data_Download-2025-06-16.csv\")\n",
        "\n",
        "\n",
        "# STEP 3: Clean-up function\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"<.*?>\", \" \", str(text))  # remove HTML tags\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    words = text.split()\n",
        "    words = [w for w in words if w not in stop_words]  # remove stopwords\n",
        "    text = \" \".join(words)  # reassemble\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # normalize whitespace\n",
        "    return text\n",
        "\n",
        "def enrich_text(row):\n",
        "    base = row[\"Family Summary\"] if pd.notna(row[\"Family Summary\"]) else row.get(\"Document Title\", \"\")\n",
        "    topic = f\"Topic: {row['Topic/Response']}\" if pd.notna(row.get(\"Topic/Response\")) else \"\"\n",
        "    timeline = f\"Events: {row['Full timeline of event types']}\" if pd.notna(row.get(\"Full timeline of event types\")) else \"\"\n",
        "    return f\"{base}. {topic}. {timeline}\"\n",
        "\n",
        "# 👷 Step 2: Apply text processing to dataset\n",
        "data[\"text_for_topic_model\"] = data.apply(enrich_text, axis=1)\n",
        "data[\"text_clean\"] = data[\"text_for_topic_model\"].apply(clean_text)\n",
        "# Check sample cleaned text\n",
        "data[\"text_clean\"].dropna().sample(3, random_state=42).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Set reproducible components\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "umap_model = UMAP(random_state=42)\n",
        "hdbscan_model = HDBSCAN(prediction_data=True)  # Enables probability outputs\n",
        "\n",
        "# Create the list of cleaned documents\n",
        "docs = data[\"text_clean\"].tolist()\n",
        "\n",
        "# Fit BERTopic with probability calculation enabled\n",
        "topic_model = BERTopic(\n",
        "    calculate_probabilities=True,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce to 10 interpretable topics\n",
        "topic_model = topic_model.reduce_topics(docs, nr_topics=10)\n",
        "\n",
        "# Recompute distributions after reduction\n",
        "topics, probs = topic_model.transform(docs)\n",
        "\n",
        "# Save top topic and top probability\n",
        "data[\"topic\"] = topics\n",
        "data[\"topic_prob\"] = [max(p) if isinstance(p, (list, np.ndarray)) else 0 for p in probs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topics based on review: [0, 3, 4, 5]\n",
        "ambition_topics = [0, 3, 4, 5]\n",
        "\n",
        "data[\"ambition_score_topic\"] = [\n",
        "    sum([p[i] for i in ambition_topics]) if isinstance(p, (list, np.ndarray)) else 0\n",
        "    for p in probs\n",
        "]\n",
        "\n",
        "def label_ambition(score, threshold_high=0.6, threshold_med=0.3):\n",
        "    if score >= threshold_high:\n",
        "        return \"High\"\n",
        "    elif score >= threshold_med:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "data[\"ambition_label\"] = data[\"ambition_score_topic\"].apply(label_ambition)\n",
        "print(data[\"ambition_score_topic\"].describe())\n",
        "print(data[\"ambition_label\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save as a new CSV file with timestamp or version label\n",
        "data_with_topics = data.copy()\n",
        "\n",
        "# Save to CSV (change path if needed)\n",
        "data_with_topics.to_csv(\"data_with_bertopic_ambition.csv\", index=False)\n",
        "\n",
        "print(\"✅ Saved to 'data_with_bertopic_ambition.csv'\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b2] --- --- --- >>> Aggregate to country-year data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Step 1: Assign document_year using First event in timeline, then overwrite ===\n",
        "data[\"document_year\"] = pd.to_datetime(data[\"First event in timeline\"], errors=\"coerce\").dt.year\n",
        "\n",
        "def overwrite_year(row):\n",
        "    if pd.notna(row.get(\"first_passed_date\")):\n",
        "        return pd.to_datetime(row[\"first_passed_date\"], errors=\"coerce\").year\n",
        "    elif pd.notna(row.get(\"last_event_date\")):\n",
        "        return pd.to_datetime(row[\"last_event_date\"], errors=\"coerce\").year\n",
        "    else:\n",
        "        return row[\"document_year\"]\n",
        "\n",
        "data[\"document_year\"] = data.apply(overwrite_year, axis=1)\n",
        "\n",
        "# === Step 2: Drop rows missing year or geography ===\n",
        "data = data.dropna(subset=[\"document_year\", \"Geographies\"])\n",
        "data[\"document_year\"] = data[\"document_year\"].astype(int)\n",
        "\n",
        "# === Step 3: Aggregate BERTopic results by country-year ===\n",
        "bert_panel = (\n",
        "    data\n",
        "    .groupby([\"Geographies\", \"document_year\"])\n",
        "    .agg(\n",
        "        num_docs=(\"Family Summary\", \"count\"),\n",
        "        mean_topic_score=(\"ambition_score_topic\", \"mean\"),\n",
        "        share_high_topic=(\"ambition_label\", lambda x: (x == \"High\").mean()),\n",
        "        share_medium_topic=(\"ambition_label\", lambda x: (x == \"Medium\").mean()),\n",
        "        share_low_topic=(\"ambition_label\", lambda x: (x == \"Low\").mean()),\n",
        "        avg_topic_prob=(\"topic_prob\", \"mean\")\n",
        "    )\n",
        "    .reset_index()\n",
        "    .sort_values([\"Geographies\", \"document_year\"])\n",
        ")\n",
        "\n",
        "# === Step 4: Round for clarity ===\n",
        "cols_to_round = [\"mean_topic_score\", \"share_high_topic\", \"share_medium_topic\", \"share_low_topic\", \"avg_topic_prob\"]\n",
        "bert_panel[cols_to_round] = bert_panel[cols_to_round].round(3)\n",
        "\n",
        "# === Step 5 (Optional): Add ISO3 codes ===\n",
        "iso_map = data[[\"Geographies\", \"Geography ISOs\"]].dropna().drop_duplicates()\n",
        "bert_panel = bert_panel.merge(iso_map, on=\"Geographies\", how=\"left\")\n",
        "\n",
        "# === Step 6: Reorder columns ===\n",
        "bert_panel.rename(columns={\"Geography ISOs\": \"iso3\", \"document_year\": \"year\"}, inplace=True)\n",
        "cols = [\"iso3\", \"year\"] + [c for c in bert_panel.columns if c not in [\"iso3\", \"year\", \"Geographies\"]]\n",
        "bert_panel = bert_panel[cols]\n",
        "\n",
        "# === Step 7: Save to CSV ===\n",
        "bert_panel.to_csv(\"bert_topic_panel.csv\", index=False)\n",
        "print(\"✅ Saved to 'bert_topic_panel.csv'\")\n",
        "\n",
        "# === Step 8: Preview ===\n",
        "bert_panel.head(10)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b3] --- --- --- >>> Summary statistics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary Statistics\n",
        "import pandas as pd\n",
        "\n",
        "# List of BERTopic-related variables\n",
        "bert_vars = [\"ambition_score_topic\", \"target_score_CPRstyle\", \"ambition_label\", \"target_type_topic\"]\n",
        "\n",
        "# Separate numeric and categorical\n",
        "numeric_vars = [col for col in bert_vars if pd.api.types.is_numeric_dtype(data[col])]\n",
        "categorical_vars = [col for col in bert_vars if col not in numeric_vars]\n",
        "\n",
        "# Numeric: summary stats\n",
        "numeric_summary = data[numeric_vars].agg(['count', 'mean', 'std', 'min', 'max']).T\n",
        "numeric_summary = numeric_summary.rename(columns={\n",
        "    \"count\": \"N\", \"mean\": \"Mean\", \"std\": \"Std. Dev.\", \"min\": \"Min\", \"max\": \"Max\"\n",
        "}).round(2)\n",
        "\n",
        "print(\"📊 Stata-style Summary for Numeric BERTopic Variables:\\n\")\n",
        "print(numeric_summary)\n",
        "\n",
        "# Categorical: tabulation\n",
        "for col in categorical_vars:\n",
        "    print(f\"\\n🧩 Stata-style Tabulation for: {col}\")\n",
        "    tab = data[col].value_counts(dropna=False).reset_index()\n",
        "    tab.columns = [col, \"Frequency\"]\n",
        "    tab[\"Percent\"] = (tab[\"Frequency\"] / tab[\"Frequency\"].sum() * 100).round(2)\n",
        "    print(tab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter BERTopic-only panel to 2016\n",
        "bert_panel_2016 = bert_panel[bert_panel[\"document_year\"] == 2016].copy()\n",
        "\n",
        "# Stata-style summary for BERTopic ambition variables\n",
        "summary_2016_bert = bert_panel_2016[[\n",
        "    \"mean_topic_score\", \"share_high_topic\", \n",
        "    \"share_netzero\", \"share_reduction\", \"share_other\", \"avg_target_score\"\n",
        "]].agg(['count', 'mean', 'std', 'min', 'max']).T\n",
        "\n",
        "summary_2016_bert = summary_2016_bert.rename(columns={\n",
        "    \"count\": \"N\", \"mean\": \"Mean\", \"std\": \"Std. Dev.\", \"min\": \"Min\", \"max\": \"Max\"\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\n📊 Stata-style Summary for BERTopic Panel Variables in 2016:\\n\")\n",
        "print(summary_2016_bert)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot: Distribution of mean_topic_score across countries in 2016\n",
        "plt.figure(figsize=(9, 5))\n",
        "plt.hist(bert_panel_2016[\"mean_topic_score\"], bins=15, edgecolor='black', color='skyblue')\n",
        "\n",
        "plt.title(\"Distribution of Mean BERTopic Ambition Scores Across Countries (2016)\", fontsize=13)\n",
        "plt.xlabel(\"Mean Topic-Based Ambition Score (per document)\", fontsize=11)\n",
        "plt.ylabel(\"Number of Countries\", fontsize=11)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Bar Chart by ISO Country Code (2016)\n",
        "# Merge ISO if needed\n",
        "bert_panel_2016 = bert_panel_2016.merge(\n",
        "    data[[\"Geographies\", \"Geography ISOs\"]].drop_duplicates(),\n",
        "    on=\"Geographies\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Sort\n",
        "bert_panel_2016_sorted = bert_panel_2016.sort_values(\"mean_topic_score\", ascending=False)\n",
        "\n",
        "# Plot\n",
        "import numpy as np\n",
        "\n",
        "x = np.arange(len(bert_panel_2016_sorted))\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.bar(x, bert_panel_2016_sorted[\"mean_topic_score\"], width=0.6, color='skyblue', edgecolor='black')\n",
        "plt.xticks(ticks=x, labels=bert_panel_2016_sorted[\"Geography ISOs\"], rotation=90, fontsize=8)\n",
        "plt.title(\"Mean Topic-Based Ambition Scores Across Countries (2016)\")\n",
        "plt.xlabel(\"Country (ISO Code)\")\n",
        "plt.ylabel(\"Topic-Based Ambition Score (Mean per Document)\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global trend from BERTopic scores\n",
        "global_trend_topic = (\n",
        "    ambition_panel\n",
        "    .groupby(\"document_year\")\n",
        "    .agg(global_avg_topic=(\"mean_topic_score\", \"mean\"))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(global_trend_topic[\"document_year\"], global_trend_topic[\"global_avg_topic\"], marker='o')\n",
        "plt.title(\"Global Average BERTopic Ambition Score Over Time\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Avg. Topic-Based Ambition Score\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [5]. [Policy Action] Semi-Supervised Machine Learning Classification - Weak Labeling\n",
        "\n",
        "I apply weak labeling to generate initial binary labels (`1 = Climate Policy Action`, `0 = Not Action`) using heuristics based on:\n",
        "- Timeline status (\"Passed/Approved\")\n",
        "- Document Type (e.g., Law, Strategy, Plan, etc.)\n",
        "- Summary verbs indicating action (e.g., implement, enforce, allocate)\n",
        "\n",
        "This allows us to build an initial labeled dataset without full manual annotation.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Extract the gold test set & remain weak training set** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📦 Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 📄 Step 2: Load dataset (adjust path if needed)\n",
        "file_path = \"Document_Data_Download-2025-06-16.csv\"  # Or  correct relative path\n",
        "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\", quotechar='\"', low_memory=False)\n",
        "\n",
        "# ✅ Step 3: Filter for rows with non-null Family Summary and Document Type\n",
        "df_filtered = df[df[\"Family Summary\"].notnull() & df[\"Document Type\"].notnull()].copy()\n",
        "\n",
        "# ✅ Step 4: Drop rare Document Types (fewer than 5 docs) to ensure valid stratification\n",
        "value_counts = df_filtered[\"Document Type\"].value_counts()\n",
        "valid_types = value_counts[value_counts >= 5].index\n",
        "df_stratified = df_filtered[df_filtered[\"Document Type\"].isin(valid_types)].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ✅ Step 5: Stratified sampling (200 samples by Document Type)\n",
        "gold_set, _ = train_test_split(\n",
        "    df_stratified,\n",
        "    stratify=df_stratified[\"Document Type\"],\n",
        "    test_size=(len(df_stratified) - 200),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Step 6: Save to CSV (optional: preview first few rows)\n",
        "gold_set.to_csv(\"gold_policy_action_sample_200.csv\", index=False)\n",
        "print(\"✅ Gold set saved to 'gold_policy_action_sample_200.csv'\")\n",
        "display(gold_set.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📦 Step 1: Filter remaining samples (exclude the gold test set)\n",
        "# Make sure both `df_stratified` and `gold_set` exist from earlier\n",
        "\n",
        "df_weak_train = df_stratified[~df_stratified.index.isin(gold_set.index)].copy()\n",
        "\n",
        "# ✅ Step 2: Save to CSV\n",
        "df_weak_train.to_csv(\"weak_train_policy_action.csv\", index=False)\n",
        "print(\"✅ Weak supervision training set saved as 'weak_train_policy_action.csv'\")\n",
        "display(df_weak_train.head())\n",
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample change:\n",
        "# Full 8974 => removing missing in [Family Summary] [Document Type], remain: 8922\n",
        "# filter rare, then extract 200 testing set, remain 8722\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b1] --- --- --- [not use] Apply weak supversing (or rule - relax condidtion)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install snorkel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 1: Load full weak training dataset (keep all columns)\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"weak_train_policy_action.csv\")\n",
        "\n",
        "# Filter only rows with valid Family Summary (retain all columns)\n",
        "df_train = df_train[df_train[\"Family Summary\"].notnull()].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from snorkel.labeling import labeling_function\n",
        "\n",
        "ABSTAIN = -1  # manually define it ( labeling functions will work without needing to import ABSTAIN.)\n",
        "ACTION = 1\n",
        "NOT_ACTION = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Define Labeling Functions (LFs)\n",
        "\n",
        "# LF1: Passed/Approved mentioned\n",
        "@labeling_function()\n",
        "def lf_passed_approved(x):\n",
        "    return ACTION if \"passed/approved\" in str(x[\"Full timeline of events (types)\"]).lower() else ABSTAIN\n",
        "\n",
        "# LF2: Strong action document types\n",
        "@labeling_function()\n",
        "def lf_strong_doc_type(x):\n",
        "    strong_types = [\"law\", \"act\", \"decree\", \"regulation\", \"executive order\",\n",
        "                    \"plan\", \"strategy\", \"action plan\", \"framework\", \"policy\", \"roadmap\"]\n",
        "    return ACTION if any(t in str(x[\"Document Type\"]).lower() for t in strong_types) else ABSTAIN\n",
        "\n",
        "# LF3: Weak doc types (non-action)\n",
        "@labeling_function()\n",
        "def lf_weak_doc_type(x):\n",
        "    weak_types = [\"submission\", \"inventory\", \"report\", \"press release\", \"communication\", \"ndc\"]\n",
        "    return NOT_ACTION if any(t in str(x[\"Document Type\"]).lower() for t in weak_types) else ABSTAIN\n",
        "\n",
        "# LF4: Action-related verbs in Family Summary\n",
        "@labeling_function()\n",
        "def lf_action_keywords(x):\n",
        "    verbs = [\"implement\", \"enforce\", \"regulate\", \"support\", \"establish\", \"provide\",\n",
        "             \"launch\", \"apply\", \"administer\", \"monitor\", \"execute\", \"fund\", \"finance\",\n",
        "             \"deliver\", \"operate\", \"incentivize\", \"reporting\", \"compliance\", \"mechanism\"]\n",
        "    text = str(x[\"Family Summary\"]).lower()\n",
        "    return ACTION if any(word in text for word in verbs) else ABSTAIN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Step 4: Apply LFs using Snorkel\n",
        "from snorkel.labeling import PandasLFApplier\n",
        "\n",
        "lfs = [lf_passed_approved, lf_strong_doc_type, lf_weak_doc_type, lf_action_keywords]\n",
        "applier = PandasLFApplier(lfs)\n",
        "\n",
        "# IMPORTANT: apply to full df with all original columns preserved\n",
        "L_train = applier.apply(df=df_train)\n",
        "\n",
        "# inspect label matrix\n",
        "import numpy as np\n",
        "print(\"Label matrix shape:\", L_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 5: Train Snorkel LabelModel and Add New Columns\n",
        "from snorkel.labeling.model import LabelModel\n",
        "\n",
        "label_model = LabelModel(cardinality=2, verbose=True)\n",
        "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100)\n",
        "\n",
        "# Add predicted hard and soft labels to full original DataFrame\n",
        "df_train[\"policy_action\"] = label_model.predict(L=L_train)\n",
        "df_train[\"action_label_prob\"] = label_model.predict_proba(L=L_train)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 6: Save Full Dataset for Future ML Use\n",
        "# All original columns + policy_action + action_label_prob\n",
        "df_train.to_csv(\"weak_labeled_policy_action_dataset.csv\", index=False)\n",
        "print(\"✅ Saved complete dataset with weak labels as 'weak_labeled_policy_action_dataset.csv'\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b2] --- --- --- [not use] Apply weak supversing (and rule - stricter condidtion) Ver 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 1: Load full weak training dataset (keep all columns)\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"weak_train_policy_action.csv\")\n",
        "\n",
        "# Filter only rows with valid Family Summary (retain all columns)\n",
        "df_train = df_train[df_train[\"Family Summary\"].notnull()].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 2: Define strict AND logic-based labeling function\n",
        "from snorkel.labeling import labeling_function\n",
        "\n",
        "ABSTAIN = -1\n",
        "ACTION = 1\n",
        "NOT_ACTION = 0\n",
        "\n",
        "@labeling_function()\n",
        "def lf_strict_policy_action(x):\n",
        "    # 1. Check Passed/Approved\n",
        "    status_check = \"passed/approved\" in str(x[\"Full timeline of events (types)\"]).lower()\n",
        "    \n",
        "    # 2. Check strong document types\n",
        "    strong_types = [\"law\", \"act\", \"decree\", \"regulation\", \"executive order\",\n",
        "                    \"plan\", \"strategy\", \"action plan\", \"framework\", \"policy\", \"roadmap\"]\n",
        "    doc_type = str(x[\"Document Type\"]).lower()\n",
        "    doc_type_check = any(t in doc_type for t in strong_types)\n",
        "\n",
        "    # 3. Check weak document types (should NOT be any)\n",
        "    weak_types = [\"submission\", \"inventory\", \"report\", \"press release\", \"communication\", \"ndc\"]\n",
        "    if any(w in doc_type for w in weak_types):\n",
        "        return NOT_ACTION\n",
        "\n",
        "    # 4. Check Family Summary for action verbs\n",
        "    verbs = [\"implement\", \"enforce\", \"regulate\", \"support\", \"establish\", \"provide\",\n",
        "             \"launch\", \"apply\", \"administer\", \"monitor\", \"execute\", \"fund\", \"finance\",\n",
        "             \"deliver\", \"operate\", \"incentivize\", \"reporting\", \"compliance\", \"mechanism,\n",
        "             \"adopt\", \n",
        "             \"]\n",
        "    summary = str(x[\"Family Summary\"]).lower()\n",
        "    text_check = any(v in summary for v in verbs)\n",
        "\n",
        "    # Final decision: all three required checks must pass\n",
        "    if status_check and doc_type_check and text_check:\n",
        "        return ACTION\n",
        "    else:\n",
        "        return ABSTAIN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 3: Apply strict LF using Snorkel\n",
        "from snorkel.labeling import PandasLFApplier\n",
        "\n",
        "lfs = [lf_strict_policy_action]\n",
        "applier = PandasLFApplier(lfs=lfs)\n",
        "L_train = applier.apply(df=df_train)\n",
        "\n",
        "# Inspect label matrix\n",
        "import numpy as np\n",
        "print(\"Label matrix shape:\", L_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 4: Train Snorkel LabelModel and Generate Labels (Skipped - dont use)\n",
        "# We have created a strict, rule-based labeling function (lf_strict_policy_action) that implements a fully deterministic AND condition: \n",
        "# Passed/Approved\n",
        "# AND Document Type is strong\n",
        "# AND not weak\n",
        "# AND Family Summary contains action verb\n",
        "# The Snorkel LabelModel is helpful only when:\n",
        "# We have multiple noisy labeling functions\n",
        "# We want Snorkel to learn how reliable each LF is\n",
        "# We want to combine votes when LFs conflict or abstain\n",
        "# Why Skipping Snorkel’s LabelModel Is OK\n",
        "# Snorkel is a tool to combine multiple noisy labels\n",
        "# But our rule is already high-precision and low-noise\n",
        "# So we are using deterministic weak labeling\n",
        "# That still counts as weak supervision — just a rule-based variant (which is valid and widely used).\n",
        "\n",
        "# Weak supervision means using imperfect, heuristic, or rule-based labels instead of (or in addition to)\n",
        "# manually annotated ground truth to train or bootstrap a machine learning model.\n",
        "\n",
        "from snorkel.labeling.model import LabelModel\n",
        "\n",
        "label_model = LabelModel(cardinality=2, verbose=True)\n",
        "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100)\n",
        "\n",
        "df_train[\"policy_action\"] = label_model.predict(L=L_train)\n",
        "df_train[\"action_label_prob\"] = label_model.predict_proba(L=L_train)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Apply the rule directly to label all rows\n",
        "df_train[\"policy_action\"] = df_train.apply(lf_strict_policy_action, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train[\"policy_action\"] = df_train[\"policy_action\"].replace(-1, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 5: Save full dataset for future use\n",
        "df_train.to_csv(\"weak_labeled_policy_action_dataset_strict.csv\", index=False)\n",
        "print(\"✅ Saved strict weak-labeled dataset as 'weak_labeled_policy_action_dataset_strict.csv'\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b3] --- --- --- [Used] Apply weak supversing (and rule - stricter condidtion) Ver 2**\n",
        "\n",
        "i. now exclude :  \"plan\", \"strategy\", \"roadmap\"\n",
        "\n",
        "ii. should check again the document type, may miss several important type: EU regulation, Act..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 1: Load full weak training dataset (keep all columns)\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"weak_train_policy_action.csv\")\n",
        "\n",
        "# Filter only rows with valid Family Summary (retain all columns)\n",
        "df_train = df_train[df_train[\"Family Summary\"].notnull()].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 2: Define strict AND logic-based labeling function\n",
        "from snorkel.labeling import labeling_function\n",
        "\n",
        "ABSTAIN = -1\n",
        "ACTION = 1\n",
        "NOT_ACTION = 0\n",
        "\n",
        "@labeling_function()\n",
        "def lf_strict_policy_action(x):\n",
        "    # 1. Check Passed/Approved\n",
        "    status_check = \"passed/approved\" in str(x[\"Full timeline of events (types)\"]).lower()\n",
        "    \n",
        "    # 2. Check strong document types\n",
        "    # now exclude :  \"plan\", \"strategy\", \"roadmap\"\n",
        "    strong_types = [\"law\", \"act\", \"decree\", \"regulation\", \"executive order\", \"order\",\n",
        "                    \"action plan\", \"framework\", \"policy\", \"directive\", \"ordinance\",\n",
        "                    \"eu directive\", \"decree law\", \"eu regulation\", \"royal decree\"]\n",
        "    doc_type = str(x[\"Document Type\"]).lower()\n",
        "    doc_type_check = any(t in doc_type for t in strong_types)\n",
        "\n",
        "    # 3. Check weak document types (should NOT be any)\n",
        "    weak_types = [\"submission\", \"inventory\", \"report\", \"press release\", \"communication\", \"ndc\"]\n",
        "    if any(w in doc_type for w in weak_types):\n",
        "        return NOT_ACTION\n",
        "\n",
        "    # 4. Check Family Summary for action verbs\n",
        "    verbs = [\"implement\", \"enforce\", \"regulate\", \"support\", \"establish\", \"provide\",\n",
        "             \"launch\", \"apply\", \"administer\", \"monitor\", \"execute\", \"fund\", \"finance\",\n",
        "             \"deliver\", \"operate\", \"incentivize\", \"reporting\", \"compliance\",\n",
        "             \"adopt\",\"reduce\", \"protect\", \"mitigate\", \"build capacity\", \"empower\", \"increase resilience\"]\n",
        "    summary = str(x[\"Family Summary\"]).lower()\n",
        "    text_check = any(v in summary for v in verbs)\n",
        "\n",
        "    # Final decision: all three required checks must pass\n",
        "    if status_check and doc_type_check and text_check:\n",
        "        return ACTION\n",
        "    else:\n",
        "        return ABSTAIN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Apply the rule directly to label all rows\n",
        "df_train[\"policy_action\"] = df_train.apply(lf_strict_policy_action, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train[\"policy_action\"] = df_train[\"policy_action\"].replace(-1, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 5: Save full dataset for future use\n",
        "df_train.to_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", index=False)\n",
        "print(\"✅ Saved strict weak-labeled dataset as 'weak_labeled_policy_action_dataset_strict2.csv'\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [6]. [Policy Action] Classification using standard Machine learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1. Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import re\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    mean_squared_error, brier_score_loss\n",
        ")\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix XGBoost training issue.\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    mean_squared_error, brier_score_loss\n",
        ")\n",
        "\n",
        "# === (h) Train Random Forest Model ===\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"✅ Random Forest model trained.\")\n",
        "\n",
        "\n",
        "# === (i) Make Predictions ===\n",
        "rf_pred = rf_model.predict(X_test)  # default threshold = 0.5\n",
        "rf_proba = rf_model.predict_proba(X_test)[:, 1]  # probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🔍 Total length of y_test:\", len(y_test))\n",
        "print(\"❌ Missing values in y_test:\", y_test.isna().sum())\n",
        "# Show rows with NaN in y_test\n",
        "missing_rows = X_test[y_test.isna()]\n",
        "display(missing_rows.head())\n",
        "\n",
        "# Optionally, show original y_test values\n",
        "print(\"🧯 Missing y_test entries:\")\n",
        "print(y_test[y_test.isna()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (j) Filter out rows with NaN in y_test ===\n",
        "valid_idx = ~y_test.isna()\n",
        "\n",
        "y_test_clean = y_test[valid_idx]\n",
        "y_pred_clean = rf_pred[valid_idx]\n",
        "y_proba_clean = rf_proba[valid_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Now run  evaluation\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss, mean_squared_error, confusion_matrix\n",
        "\n",
        "\n",
        "# === (k) Evaluate ===\n",
        "print(f\"✅ Accuracy: {accuracy_score(y_test_clean, y_pred_clean):.3f}\")\n",
        "print(f\"✅ AUC: {roc_auc_score(y_test_clean, y_proba_clean):.3f}\")\n",
        "print(f\"✅ Brier Score: {brier_score_loss(y_test_clean, y_proba_clean):.4f}\")\n",
        "print(f\"✅ MSE: {mean_squared_error(y_test_clean, y_proba_clean):.4f}\")\n",
        "print(\"✅ Confusion Matrix:\\n\", confusion_matrix(y_test_clean, y_pred_clean))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (ii) Enhanced Confusion Matrix (Stata-style) ===\n",
        "cm = confusion_matrix(y_test_clean, y_pred_clean, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"leaver_binary |   pred_leave\")\n",
        "print(\"       ary     |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (iii) Scalar-style metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (l) Save Predictions ===\n",
        "test_clean = test[valid_idx].copy()\n",
        "test_clean[\"rf_pred\"] = y_pred_clean\n",
        "test_clean[\"rf_prob\"] = y_proba_clean\n",
        "test_clean.to_csv(\"randomforest_predictions_policyaction.csv\", index=False)\n",
        "print(\"📁 Exported predictions to: randomforest_predictions_policyaction.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(len(test))           # Full test set\n",
        "print(len(y_pred))         # Should match test if no filtering\n",
        "print(len(y_pred_clean))   # Matches only valid_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Python Code for ROC Threshold Evaluation and Export**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === (1) Load final predictions ===\n",
        "df = pd.read_csv(\"randomforest_predictions_policyaction.csv\")\n",
        "\n",
        "# === (2) Define column names\n",
        "score_col = \"rf_prob\"         # Probabilities from XGBoost\n",
        "actual_col = \"policy_action\"   # True label from gold test set\n",
        "\n",
        "# === (3) Filter clean data ===\n",
        "df = df[[score_col, actual_col]].dropna()\n",
        "df['score_round'] = df[score_col].round(4)  # Round for consistency\n",
        "\n",
        "# === (4) Prepare ROC table ===\n",
        "thresholds = np.unique(df['score_round'])\n",
        "results = []\n",
        "\n",
        "P = (df[actual_col] == 1).sum()\n",
        "N = (df[actual_col] == 0).sum()\n",
        "\n",
        "for thresh in thresholds:\n",
        "    pred = (df[score_col] > thresh).astype(int)\n",
        "    TP = ((df[actual_col] == 1) & (pred == 1)).sum()\n",
        "    FP = ((df[actual_col] == 0) & (pred == 1)).sum()\n",
        "    FN = ((df[actual_col] == 1) & (pred == 0)).sum()\n",
        "    TN = ((df[actual_col] == 0) & (pred == 0)).sum()\n",
        "\n",
        "    TPR = TP / P if P > 0 else 0\n",
        "    FPR = FP / N if N > 0 else 0\n",
        "    distance = np.sqrt((1 - TPR) ** 2 + (FPR) ** 2)\n",
        "\n",
        "    results.append({\n",
        "        \"cutoff\": thresh,\n",
        "        \"TPR\": TPR,\n",
        "        \"FPR\": FPR,\n",
        "        \"distance\": distance,\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"TN\": TN,\n",
        "        \"FN\": FN\n",
        "    })\n",
        "\n",
        "# === (5) Convert to DataFrame ===\n",
        "roc_df = pd.DataFrame(results)\n",
        "\n",
        "# === (6) Find best threshold ===\n",
        "best_row = roc_df.loc[roc_df['distance'].idxmin()]\n",
        "best_cutoff = best_row['cutoff']\n",
        "\n",
        "print(\"\\n🎯 Optimal Threshold Results\")\n",
        "print(f\"Best cutoff       : {best_cutoff:.4f}\")\n",
        "print(f\"True Positive Rate: {best_row['TPR']:.4f}\")\n",
        "print(f\"False Positive Rate: {best_row['FPR']:.4f}\")\n",
        "print(f\"Min Distance      : {best_row['distance']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (7) Save all ROC points to CSV for review ===\n",
        "roc_df.to_csv(\"rocpoints_randomforest.csv\", index=False)\n",
        "print(\"📁 All thresholds and ROC stats saved to: rocpoints_randomforest.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# === (1) Apply Best Cutoff ===\n",
        "best_cutoff = 0.3528  # Replace with actual best threshold\n",
        "df[\"final_pred\"] = (df[\"rf_prob\"] > best_cutoff).astype(int)\n",
        "\n",
        "# Extract clean y_true and y_pred (already NaNs dropped earlier)\n",
        "y_true = df[\"policy_action\"]\n",
        "y_pred = df[\"final_pred\"]\n",
        "\n",
        "# === (2) Compute Confusion Matrix ===\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "# === (3) Stata-style Table Output ===\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"policy_action |   predicted\")\n",
        "print(\"             |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (4) Scalar Metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2. Xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import re\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    mean_squared_error, brier_score_loss\n",
        ")\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that XGBoost cannot handle\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix XGBoost training issue.\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (h) Train XGBoost Model ===\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.2,\n",
        "    n_jobs=-1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# === (i) Predict and Evaluate ===\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🔍 Total length of y_test:\", len(y_test))\n",
        "print(\"❌ Missing values in y_test:\", y_test.isna().sum())\n",
        "# Show rows with NaN in y_test\n",
        "missing_rows = X_test[y_test.isna()]\n",
        "display(missing_rows.head())\n",
        "\n",
        "# Optionally, show original y_test values\n",
        "print(\"🧯 Missing y_test entries:\")\n",
        "print(y_test[y_test.isna()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter out rows where y_test is NaN\n",
        "valid_idx = ~y_test.isna()\n",
        "\n",
        "# Filter both y_test and y_pred / y_proba accordingly\n",
        "y_test_clean = y_test[valid_idx]\n",
        "y_pred_clean = y_pred[valid_idx]\n",
        "y_proba_clean = y_proba[valid_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Now run evaluation\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss, mean_squared_error, confusion_matrix\n",
        "\n",
        "print(f\"✅ Accuracy: {accuracy_score(y_test_clean, y_pred_clean):.3f}\")\n",
        "print(f\"✅ AUC: {roc_auc_score(y_test_clean, y_proba_clean):.3f}\")\n",
        "print(f\"✅ Brier Score: {brier_score_loss(y_test_clean, y_proba_clean):.4f}\")\n",
        "print(f\"✅ MSE: {mean_squared_error(y_test_clean, y_proba_clean):.4f}\")\n",
        "print(\"✅ Confusion Matrix:\\n\", confusion_matrix(y_test_clean, y_pred_clean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (ii) Enhanced Confusion Matrix (Stata-style) ===\n",
        "cm = confusion_matrix(y_test_clean, y_pred_clean, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"leaver_binary |   pred_leave\")\n",
        "print(\"       ary     |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (iii) Scalar-style metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (j) Save predictions ===\n",
        "test_clean = test[valid_idx].copy()\n",
        "test_clean[\"xgb_pred\"] = y_pred_clean\n",
        "test_clean[\"xgb_prob\"] = y_proba_clean\n",
        "test_clean.to_csv(\"xgboost_predictions_policyaction.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(len(test))           # Full test set\n",
        "print(len(y_pred))         # Should match test if no filtering\n",
        "print(len(y_pred_clean))   # Matches only valid_idx\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Python Code for ROC Threshold Evaluation and Export**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === (1) Load final predictions ===\n",
        "df = pd.read_csv(\"xgboost_predictions_policyaction.csv\")\n",
        "\n",
        "# === (2) Define column names\n",
        "score_col = \"xgb_prob\"         # Probabilities from XGBoost\n",
        "actual_col = \"policy_action\"   # True label from gold test set\n",
        "\n",
        "# === (3) Filter clean data ===\n",
        "df = df[[score_col, actual_col]].dropna()\n",
        "df['score_round'] = df[score_col].round(4)  # Round for consistency\n",
        "\n",
        "# === (4) Prepare ROC table ===\n",
        "thresholds = np.unique(df['score_round'])\n",
        "results = []\n",
        "\n",
        "P = (df[actual_col] == 1).sum()\n",
        "N = (df[actual_col] == 0).sum()\n",
        "\n",
        "for thresh in thresholds:\n",
        "    pred = (df[score_col] > thresh).astype(int)\n",
        "    TP = ((df[actual_col] == 1) & (pred == 1)).sum()\n",
        "    FP = ((df[actual_col] == 0) & (pred == 1)).sum()\n",
        "    FN = ((df[actual_col] == 1) & (pred == 0)).sum()\n",
        "    TN = ((df[actual_col] == 0) & (pred == 0)).sum()\n",
        "\n",
        "    TPR = TP / P if P > 0 else 0\n",
        "    FPR = FP / N if N > 0 else 0\n",
        "    distance = np.sqrt((1 - TPR) ** 2 + (FPR) ** 2)\n",
        "\n",
        "    results.append({\n",
        "        \"cutoff\": thresh,\n",
        "        \"TPR\": TPR,\n",
        "        \"FPR\": FPR,\n",
        "        \"distance\": distance,\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"TN\": TN,\n",
        "        \"FN\": FN\n",
        "    })\n",
        "\n",
        "# === (5) Convert to DataFrame ===\n",
        "roc_df = pd.DataFrame(results)\n",
        "\n",
        "# === (6) Find best threshold ===\n",
        "best_row = roc_df.loc[roc_df['distance'].idxmin()]\n",
        "best_cutoff = best_row['cutoff']\n",
        "\n",
        "print(\"\\n🎯 Optimal Threshold Results\")\n",
        "print(f\"Best cutoff       : {best_cutoff:.4f}\")\n",
        "print(f\"True Positive Rate: {best_row['TPR']:.4f}\")\n",
        "print(f\"False Positive Rate: {best_row['FPR']:.4f}\")\n",
        "print(f\"Min Distance      : {best_row['distance']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (7) Save all ROC points to CSV for review ===\n",
        "roc_df.to_csv(\"rocpoints_xgboost.csv\", index=False)\n",
        "print(\"📁 All thresholds and ROC stats saved to: rocpoints_xgboost.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# === (1) Apply Best Cutoff ===\n",
        "best_cutoff = 0.2107  # Replace with actual best threshold\n",
        "df[\"final_pred\"] = (df[\"xgb_prob\"] > best_cutoff).astype(int)\n",
        "\n",
        "# Extract clean y_true and y_pred (already NaNs dropped earlier)\n",
        "y_true = df[\"policy_action\"]\n",
        "y_pred = df[\"final_pred\"]\n",
        "\n",
        "# === (2) Compute Confusion Matrix ===\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "# === (3) Stata-style Table Output ===\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"policy_action |   predicted\")\n",
        "print(\"             |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (4) Scalar Metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Combine test metadata with model predictions\n",
        "results_df = X_test.copy()\n",
        "results_df['actual'] = y_test\n",
        "results_df['predicted'] = y_pred\n",
        "\n",
        "# Add metadata columns from original test set\n",
        "results_df['Document ID'] = test['Document ID']\n",
        "results_df['Family ID'] = test['Family ID']\n",
        "results_df['Document Type'] = test['Document Type']\n",
        "\n",
        "# Identify False Positives: predicted 1, actual 0\n",
        "false_positives = results_df[(results_df['predicted'] == 1) & (results_df['actual'] == 0)]\n",
        "\n",
        "# Identify False Negatives: predicted 0, actual 1\n",
        "false_negatives = results_df[(results_df['predicted'] == 0) & (results_df['actual'] == 1)]\n",
        "\n",
        "# Save results with identifying columns\n",
        "false_positives.to_csv(\"false_positives_with_IDs.csv\", index=False)\n",
        "false_negatives.to_csv(\"false_negatives_with_IDs.csv\", index=False)\n",
        "\n",
        "# Print summaries\n",
        "print(\"False Positives (Predicted 1, Actual 0):\", false_positives.shape[0])\n",
        "print(\"False Negatives (Predicted 0, Actual 1):\", false_negatives.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "false_positives.to_csv(\"false_positives.csv\", index=False)\n",
        "false_negatives.to_csv(\"false_negatives.csv\", index=False)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3. Neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    mean_squared_error, brier_score_loss\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix XGBoost training issue.\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- Extra cleaning before Neural network**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (j) Normalize features ===\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[e] --- --- --- Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLPClassifier - based pipeline (align with simple basic RF & xgboost) - dont use\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "nn_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(100,),   # One hidden layer with 100 neurons\n",
        "    activation='relu',           # ReLU activation\n",
        "    solver='adam',               # Adaptive gradient descent\n",
        "    alpha=0.0001,                # L2 regularization\n",
        "    learning_rate='adaptive',    # Adjusts learning rate based on validation loss\n",
        "    max_iter=500,                # Enough iterations to converge\n",
        "    random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keras deep neural network (DNN) \n",
        "# === (k) Define and train Neural Network ===\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),  \n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100, batch_size=64, callbacks=[early_stop], verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (l) Predict ===\n",
        "\n",
        "# ✅ Step 1: Identify valid test indices (non-missing labels)\n",
        "valid_idx = ~y_test.isna()\n",
        "\n",
        "# ✅ Step 2: Filter out missing test labels\n",
        "y_test_clean = y_test[valid_idx].astype(int)\n",
        "X_test_clean = X_test_scaled[valid_idx]\n",
        "\n",
        "# ✅ Step 3: Predict probabilities and class labels\n",
        "y_proba = model.predict(X_test_clean, verbose=0).flatten()  # Get predicted probabilities\n",
        "y_pred = (y_proba > 0.5).astype(int)                         # Default threshold: 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 1: Diagnose test set\n",
        "print(\"🔍 Total length of y_test:\", len(y_test))\n",
        "print(\"❌ Missing values in y_test:\", y_test.isna().sum())\n",
        "\n",
        "# Optional: show problematic test rows (i.e., where no label is available)\n",
        "missing_rows = X_test[y_test.isna()]\n",
        "display(missing_rows.head())\n",
        "\n",
        "print(\"🧯 Missing y_test entries:\")\n",
        "print(y_test[y_test.isna()])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 2: Filter out NaN labels from y_test only\n",
        "valid_idx = ~y_test.isna()\n",
        "\n",
        "y_test_clean = y_test[valid_idx].astype(int)  # Ensure binary labels for evaluation\n",
        "y_pred_clean = y_pred             # Already of length 200\n",
        "y_proba_clean = y_proba           # Already of length 200\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (m) Evaluate Neural Network Model ===\n",
        "print(f\"✅ Accuracy:     {accuracy_score(y_test_clean, y_pred):.3f}\")\n",
        "print(f\"✅ AUC:          {roc_auc_score(y_test_clean, y_proba):.3f}\")\n",
        "print(f\"✅ Brier Score:  {brier_score_loss(y_test_clean, y_proba):.4f}\")\n",
        "print(f\"✅ MSE:          {mean_squared_error(y_test_clean, y_proba):.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test_clean, y_pred)\n",
        "print(\"✅ Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Optional: Unpack and display confusion matrix in labeled form\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "print(f\"   TN: {TN}, FP: {FP}, FN: {FN}, TP: {TP}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (ii) Enhanced Confusion Matrix (Stata-style) ===\n",
        "cm = confusion_matrix(y_test_clean, y_pred_clean, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"policy_action |   predicted\")\n",
        "print(\"              |     0        1     |   Total\")\n",
        "print(\"--------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"              | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"              | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"--------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total   | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"              | {col_perc_str} |   100.00\")\n",
        "print(\"              |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (iii) Scalar-style metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (n) Save Results ===\n",
        "test_clean = test.loc[valid_idx].copy()\n",
        "test_clean[\"nn_pred\"] = y_pred\n",
        "test_clean[\"nn_prob\"] = y_proba\n",
        "test_clean.to_csv(\"neuralnet_predictions_policyaction.csv\", index=False)\n",
        "print(\"📁 Predictions saved to neuralnet_predictions_policyaction.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Python Code for ROC Threshold Evaluation and Export**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === (1) Load final predictions ===\n",
        "df = pd.read_csv(\"neuralnet_predictions_policyaction.csv\")\n",
        "\n",
        "# === (2) Define column names\n",
        "score_col = \"nn_prob\"         # Probabilities from XGBoost\n",
        "actual_col = \"policy_action\"   # True label from gold test set\n",
        "\n",
        "# === (3) Filter clean data ===\n",
        "df = df[[score_col, actual_col]].dropna()\n",
        "df['score_round'] = df[score_col].round(4)  # Round for consistency\n",
        "\n",
        "# === (4) Prepare ROC table ===\n",
        "thresholds = np.unique(df['score_round'])\n",
        "results = []\n",
        "\n",
        "P = (df[actual_col] == 1).sum()\n",
        "N = (df[actual_col] == 0).sum()\n",
        "\n",
        "for thresh in thresholds:\n",
        "    pred = (df[score_col] > thresh).astype(int)\n",
        "    TP = ((df[actual_col] == 1) & (pred == 1)).sum()\n",
        "    FP = ((df[actual_col] == 0) & (pred == 1)).sum()\n",
        "    FN = ((df[actual_col] == 1) & (pred == 0)).sum()\n",
        "    TN = ((df[actual_col] == 0) & (pred == 0)).sum()\n",
        "\n",
        "    TPR = TP / P if P > 0 else 0\n",
        "    FPR = FP / N if N > 0 else 0\n",
        "    distance = np.sqrt((1 - TPR) ** 2 + (FPR) ** 2)\n",
        "\n",
        "    results.append({\n",
        "        \"cutoff\": thresh,\n",
        "        \"TPR\": TPR,\n",
        "        \"FPR\": FPR,\n",
        "        \"distance\": distance,\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"TN\": TN,\n",
        "        \"FN\": FN\n",
        "    })\n",
        "\n",
        "# === (5) Convert to DataFrame ===\n",
        "roc_df = pd.DataFrame(results)\n",
        "\n",
        "# === (6) Find best threshold ===\n",
        "best_row = roc_df.loc[roc_df['distance'].idxmin()]\n",
        "best_cutoff = best_row['cutoff']\n",
        "\n",
        "print(\"\\n🎯 Optimal Threshold Results\")\n",
        "print(f\"Best cutoff       : {best_cutoff:.4f}\")\n",
        "print(f\"True Positive Rate: {best_row['TPR']:.4f}\")\n",
        "print(f\"False Positive Rate: {best_row['FPR']:.4f}\")\n",
        "print(f\"Min Distance      : {best_row['distance']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (7) Save all ROC points to CSV for review ===\n",
        "roc_df.to_csv(\"rocpoints_neuralnet.csv\", index=False)\n",
        "print(\"📁 All thresholds and ROC stats saved to: rocpoints_neuralnet.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# === (1) Apply Best Cutoff ===\n",
        "best_cutoff =  0.2518  # Replace with  actual best threshold\n",
        "df[\"final_pred\"] = (df[\"nn_prob\"] > best_cutoff).astype(int)\n",
        "\n",
        "# Extract clean y_true and y_pred (already NaNs dropped earlier)\n",
        "y_true = df[\"policy_action\"]\n",
        "y_pred = df[\"final_pred\"]\n",
        "\n",
        "# === (2) Compute Confusion Matrix ===\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "# === (3) Stata-style Table Output ===\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"policy_action |   predicted\")\n",
        "print(\"             |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (4) Scalar Metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [7] Classification using Balance Machine learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1. Balance Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, brier_score_loss\n",
        "\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix XGBoost training issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    mean_squared_error, brier_score_loss\n",
        ")\n",
        "\n",
        "# === (h) Train Balanced Random Forest Classifier ===\n",
        "brf = BalancedRandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    min_samples_split=4,\n",
        "    sampling_strategy='auto',\n",
        "    replacement=True,\n",
        "    bootstrap=False,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "brf.fit(X_train, y_train)\n",
        "print(\"✅ Balanced Random Forest trained.\")\n",
        "\n",
        "\n",
        "# === (i) Make Predictions ===\n",
        "y_pred = brf.predict(X_test)\n",
        "y_prob = brf.predict_proba(X_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🔍 Total length of y_test:\", len(y_test))\n",
        "print(\"❌ Missing values in y_test:\", y_test.isna().sum())\n",
        "# Show rows with NaN in y_test\n",
        "missing_rows = X_test[y_test.isna()]\n",
        "display(missing_rows.head())\n",
        "\n",
        "# Optionally, show original y_test values\n",
        "print(\"🧯 Missing y_test entries:\")\n",
        "print(y_test[y_test.isna()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (j) Filter out rows with NaN in y_test ===\n",
        "valid_idx = ~y_test.isna()\n",
        "\n",
        "y_test_clean = y_test[valid_idx]\n",
        "y_pred_clean = y_pred[valid_idx]\n",
        "y_proba_clean = y_prob[valid_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Now run  evaluation\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss, mean_squared_error, confusion_matrix\n",
        "\n",
        "\n",
        "# === (k) Evaluate ===\n",
        "print(f\"✅ Accuracy: {accuracy_score(y_test_clean, y_pred_clean):.3f}\")\n",
        "print(f\"✅ AUC: {roc_auc_score(y_test_clean, y_proba_clean):.3f}\")\n",
        "print(f\"✅ Brier Score: {brier_score_loss(y_test_clean, y_proba_clean):.4f}\")\n",
        "print(f\"✅ MSE: {mean_squared_error(y_test_clean, y_proba_clean):.4f}\")\n",
        "print(\"✅ Confusion Matrix:\\n\", confusion_matrix(y_test_clean, y_pred_clean))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (ii) Enhanced Confusion Matrix (Stata-style) ===\n",
        "cm = confusion_matrix(y_test_clean, y_pred_clean, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"leaver_binary |   pred_leave\")\n",
        "print(\"       ary     |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (iii) Scalar-style metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (l) Save Predictions ===\n",
        "test_clean = test[valid_idx].copy()\n",
        "test_clean[\"brf_pred\"] = y_pred_clean\n",
        "test_clean[\"brf_prob\"] = y_proba_clean\n",
        "\n",
        "test_clean.to_csv(\"balancedrf_predictions_policyaction.csv\", index=False)\n",
        "print(\"📁 Exported predictions to: balancedrf_predictions_policyaction.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(len(test))           # Full test set\n",
        "print(len(y_pred))         # Should match test if no filtering\n",
        "print(len(y_pred_clean))   # Matches only valid_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Python Code for ROC Threshold Evaluation and Export**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === (1) Load final predictions ===\n",
        "df = pd.read_csv(\"balancedrf_predictions_policyaction.csv\")\n",
        "\n",
        "# === (2) Define column names\n",
        "score_col = \"brf_prob\"         # Probabilities from XGBoost\n",
        "actual_col = \"policy_action\"   # True label from gold test set\n",
        "\n",
        "# === (3) Filter clean data ===\n",
        "df = df[[score_col, actual_col]].dropna()\n",
        "df['score_round'] = df[score_col].round(4)  # Round for consistency\n",
        "\n",
        "# === (4) Prepare ROC table ===\n",
        "thresholds = np.unique(df['score_round'])\n",
        "results = []\n",
        "\n",
        "P = (df[actual_col] == 1).sum()\n",
        "N = (df[actual_col] == 0).sum()\n",
        "\n",
        "for thresh in thresholds:\n",
        "    pred = (df[score_col] > thresh).astype(int)\n",
        "    TP = ((df[actual_col] == 1) & (pred == 1)).sum()\n",
        "    FP = ((df[actual_col] == 0) & (pred == 1)).sum()\n",
        "    FN = ((df[actual_col] == 1) & (pred == 0)).sum()\n",
        "    TN = ((df[actual_col] == 0) & (pred == 0)).sum()\n",
        "\n",
        "    TPR = TP / P if P > 0 else 0\n",
        "    FPR = FP / N if N > 0 else 0\n",
        "    distance = np.sqrt((1 - TPR) ** 2 + (FPR) ** 2)\n",
        "\n",
        "    results.append({\n",
        "        \"cutoff\": thresh,\n",
        "        \"TPR\": TPR,\n",
        "        \"FPR\": FPR,\n",
        "        \"distance\": distance,\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"TN\": TN,\n",
        "        \"FN\": FN\n",
        "    })\n",
        "\n",
        "# === (5) Convert to DataFrame ===\n",
        "roc_df = pd.DataFrame(results)\n",
        "\n",
        "# === (6) Find best threshold ===\n",
        "best_row = roc_df.loc[roc_df['distance'].idxmin()]\n",
        "best_cutoff = best_row['cutoff']\n",
        "\n",
        "print(\"\\n🎯 Optimal Threshold Results\")\n",
        "print(f\"Best cutoff       : {best_cutoff:.4f}\")\n",
        "print(f\"True Positive Rate: {best_row['TPR']:.4f}\")\n",
        "print(f\"False Positive Rate: {best_row['FPR']:.4f}\")\n",
        "print(f\"Min Distance      : {best_row['distance']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (7) Save all ROC points to CSV for review ===\n",
        "roc_df.to_csv(\"rocpoints_BRF.csv\", index=False)\n",
        "print(\"📁 All thresholds and ROC stats saved to: rocpoints_BRF.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# === (1) Apply Best Cutoff ===\n",
        "best_cutoff = 0.3432  # Replace with the actual best threshold\n",
        "df[\"final_pred\"] = (df[\"brf_prob\"] > best_cutoff).astype(int)\n",
        "\n",
        "# Extract clean y_true and y_pred (already NaNs dropped earlier)\n",
        "y_true = df[\"policy_action\"]\n",
        "y_pred = df[\"final_pred\"]\n",
        "\n",
        "# === (2) Compute Confusion Matrix ===\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "# === (3) Stata-style Table Output ===\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"policy_action |   predicted\")\n",
        "print(\"             |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (4) Scalar Metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2. Balance Random forest with Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, brier_score_loss\n",
        "\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix not useful issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# === (a) Cross-validation Settings ===\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# === (b) Store results ===\n",
        "metrics = {\n",
        "    'accuracy': [], 'precision': [], 'recall': [],\n",
        "    'f1': [], 'auc': [], 'specificity': []\n",
        "}\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "# === (c) Cross-validation loop ===\n",
        "fold_num = 1\n",
        "for train_idx, val_idx in cv.split(X_train, y_train):\n",
        "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    # Balanced Random Forest for this fold\n",
        "    brf_cv = BalancedRandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=7,\n",
        "        min_samples_split=4,\n",
        "        sampling_strategy='auto',\n",
        "        replacement=True,\n",
        "        bootstrap=False,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "    brf_cv.fit(X_tr, y_tr)\n",
        "\n",
        "    # Predict using default threshold (0.5)\n",
        "    y_proba = brf_cv.predict_proba(X_val)[:, 1]\n",
        "    y_pred = brf_cv.predict(X_val)\n",
        "\n",
        "    # Save out-of-fold predictions\n",
        "    all_y_true.extend(y_val)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "    # Confusion matrix and metrics for this fold\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "    metrics['accuracy'].append(accuracy_score(y_val, y_pred))\n",
        "    metrics['precision'].append(precision_score(y_val, y_pred, zero_division=0))\n",
        "    metrics['recall'].append(recall_score(y_val, y_pred))\n",
        "    metrics['f1'].append(f1_score(y_val, y_pred))\n",
        "    metrics['auc'].append(roc_auc_score(y_val, y_proba))\n",
        "    metrics['specificity'].append(TN / (TN + FP))\n",
        "\n",
        "    print(f\"✅ Fold {fold_num} completed.\")\n",
        "    fold_num += 1\n",
        "\n",
        "# === (d) Print average ± std of metrics ===\n",
        "print(\"\\n=== 📊 5-Fold CV Metrics (Default Cutoff = 0.5) ===\")\n",
        "for key in metrics:\n",
        "    mean = np.mean(metrics[key])\n",
        "    std = np.std(metrics[key])\n",
        "    print(f\"{key.capitalize():<12}: {mean:.4f} ± {std:.4f}\")\n",
        "\n",
        "# === (e) Final aggregated evaluation ===\n",
        "print(\"\\n=== 🔎 Final Aggregated Performance on All Out-of-Fold Predictions ===\")\n",
        "final_cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "TN, FP, FN, TP = final_cm.ravel()\n",
        "print(\"Confusion Matrix:\\n\", final_cm)\n",
        "print(f\"Specificity: {TN / (TN + FP):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_y_true, all_y_pred, digits=4))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3. Imbalance Weight Xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, brier_score_loss\n",
        "\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix XGBoost training issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# === (h) Compute imbalance weight ===\n",
        "n_pos = (y_train == 1).sum()\n",
        "n_neg = (y_train == 0).sum()\n",
        "scale_pos_weight = n_neg / n_pos\n",
        "print(f\"✅ Calculated scale_pos_weight: {scale_pos_weight:.4f}\")\n",
        "\n",
        "# === (i) Train Imbalanced XGBoost Model ===\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.2,\n",
        "    n_jobs=-1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    scale_pos_weight=scale_pos_weight\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"✅ Weighted XGBoost model trained.\")\n",
        "\n",
        "\n",
        "# === (i) Predict and Evaluate ===\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🔍 Total length of y_test:\", len(y_test))\n",
        "print(\"❌ Missing values in y_test:\", y_test.isna().sum())\n",
        "# Show rows with NaN in y_test\n",
        "missing_rows = X_test[y_test.isna()]\n",
        "display(missing_rows.head())\n",
        "\n",
        "# Optionally, show original y_test values\n",
        "print(\"🧯 Missing y_test entries:\")\n",
        "print(y_test[y_test.isna()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (j) Filter out rows with NaN in y_test ===\n",
        "valid_idx = ~y_test.isna()\n",
        "\n",
        "# Apply mask to clean targets and predictions\n",
        "y_test_clean = y_test[valid_idx]\n",
        "y_pred_clean = y_pred[valid_idx]\n",
        "y_proba_clean = y_proba[valid_idx]\n",
        "\n",
        "# Log results\n",
        "print(f\"✅ Valid y_test entries: {valid_idx.sum()} / {len(y_test)}\")\n",
        "print(f\"✅ Cleaned predictions and probabilities aligned.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Now run evaluation\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score,\n",
        "    brier_score_loss, mean_squared_error,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# === (k) Evaluation Metrics ===\n",
        "print(\"\\n=== 📊 Evaluation Results (Default Threshold = 0.5) ===\")\n",
        "print(f\"✅ Accuracy       : {accuracy_score(y_test_clean, y_pred_clean):.3f}\")\n",
        "print(f\"✅ AUC            : {roc_auc_score(y_test_clean, y_proba_clean):.3f}\")\n",
        "print(f\"✅ Brier Score    : {brier_score_loss(y_test_clean, y_proba_clean):.4f}\")\n",
        "print(f\"✅ MSE            : {mean_squared_error(y_test_clean, y_proba_clean):.4f}\")\n",
        "\n",
        "# === (l) Confusion Matrix ===\n",
        "print(\"\\n✅ Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_clean, y_pred_clean, labels=[0, 1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (ii) Enhanced Confusion Matrix (Stata-style) ===\n",
        "cm = confusion_matrix(y_test_clean, y_pred_clean, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"leaver_binary |   pred_leave\")\n",
        "print(\"       ary     |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (iii) Scalar-style metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (l) Save Predictions ===\n",
        "test_clean = test[valid_idx].copy()\n",
        "test_clean[\"xgb_pred\"] = y_pred_clean\n",
        "test_clean[\"xgb_prob\"] = y_proba_clean\n",
        "\n",
        "test_clean.to_csv(\"imbaxgboost_predictions_policyaction.csv\", index=False)\n",
        "print(\"📁 Exported predictions to: imbaxgboost_predictions_policyaction.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Python Code for ROC Threshold Evaluation and Export**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === (1) Load final predictions ===\n",
        "df = pd.read_csv(\"imbaxgboost_predictions_policyaction.csv\")\n",
        "\n",
        "# === (2) Define column names\n",
        "score_col = \"xgb_prob\"         # Probabilities from XGBoost\n",
        "actual_col = \"policy_action\"   # True label from gold test set\n",
        "\n",
        "# === (3) Filter clean data ===\n",
        "df = df[[score_col, actual_col]].dropna()\n",
        "df['score_round'] = df[score_col].round(4)  # Round for consistency\n",
        "\n",
        "# === (4) Prepare ROC table ===\n",
        "thresholds = np.unique(df['score_round'])\n",
        "results = []\n",
        "\n",
        "P = (df[actual_col] == 1).sum()\n",
        "N = (df[actual_col] == 0).sum()\n",
        "\n",
        "for thresh in thresholds:\n",
        "    pred = (df[score_col] > thresh).astype(int)\n",
        "    TP = ((df[actual_col] == 1) & (pred == 1)).sum()\n",
        "    FP = ((df[actual_col] == 0) & (pred == 1)).sum()\n",
        "    FN = ((df[actual_col] == 1) & (pred == 0)).sum()\n",
        "    TN = ((df[actual_col] == 0) & (pred == 0)).sum()\n",
        "\n",
        "    TPR = TP / P if P > 0 else 0\n",
        "    FPR = FP / N if N > 0 else 0\n",
        "    distance = np.sqrt((1 - TPR) ** 2 + (FPR) ** 2)\n",
        "\n",
        "    results.append({\n",
        "        \"cutoff\": thresh,\n",
        "        \"TPR\": TPR,\n",
        "        \"FPR\": FPR,\n",
        "        \"distance\": distance,\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"TN\": TN,\n",
        "        \"FN\": FN\n",
        "    })\n",
        "\n",
        "# === (5) Convert to DataFrame ===\n",
        "roc_df = pd.DataFrame(results)\n",
        "\n",
        "# === (6) Find best threshold ===\n",
        "best_row = roc_df.loc[roc_df['distance'].idxmin()]\n",
        "best_cutoff = best_row['cutoff']\n",
        "\n",
        "print(\"\\n🎯 Optimal Threshold Results\")\n",
        "print(f\"Best cutoff       : {best_cutoff:.4f}\")\n",
        "print(f\"True Positive Rate: {best_row['TPR']:.4f}\")\n",
        "print(f\"False Positive Rate: {best_row['FPR']:.4f}\")\n",
        "print(f\"Min Distance      : {best_row['distance']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (7) Save all ROC points to CSV for review ===\n",
        "roc_df.to_csv(\"rocpoints_ImbaXgboost.csv\", index=False)\n",
        "print(\"📁 All thresholds and ROC stats saved to: rocpoints_ImbaXgboost.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# === (1) Apply Best Cutoff ===\n",
        "best_cutoff = 0.3746  # Replace with the actual best threshold\n",
        "df[\"final_pred\"] = (df[\"xgb_prob\"] > best_cutoff).astype(int)\n",
        "\n",
        "# Extract clean y_true and y_pred (already NaNs dropped earlier)\n",
        "y_true = df[\"policy_action\"]\n",
        "y_pred = df[\"final_pred\"]\n",
        "\n",
        "# === (2) Compute Confusion Matrix ===\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "# === (3) Stata-style Table Output ===\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"policy_action |   predicted\")\n",
        "print(\"             |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (4) Scalar Metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4. Imbalance Xgboost with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, brier_score_loss\n",
        "\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix not useful issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, confusion_matrix\n",
        ")\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "# === (a) Cross-validation setup ===\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "print(f\"✅ Calculated scale_pos_weight: {scale_pos_weight:.4f}\")\n",
        "\n",
        "# === (b) Storage for metrics ===\n",
        "metrics = {\n",
        "    'accuracy': [], 'precision': [], 'recall': [],\n",
        "    'f1': [], 'auc': [], 'specificity': []\n",
        "}\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "# === (c) Cross-validation loop ===\n",
        "fold_num = 1\n",
        "for train_idx, val_idx in cv.split(X_train, y_train):\n",
        "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    model_cv = xgb.XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.2,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='logloss',\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        use_label_encoder=False,\n",
        "        random_state=42\n",
        "    )\n",
        "    model_cv.fit(X_tr, y_tr)\n",
        "\n",
        "    y_proba = model_cv.predict_proba(X_val)[:, 1]\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "    # Save out-of-fold predictions\n",
        "    all_y_true.extend(y_val)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "    metrics['accuracy'].append(accuracy_score(y_val, y_pred))\n",
        "    metrics['precision'].append(precision_score(y_val, y_pred, zero_division=0))\n",
        "    metrics['recall'].append(recall_score(y_val, y_pred))\n",
        "    metrics['f1'].append(f1_score(y_val, y_pred))\n",
        "    metrics['auc'].append(roc_auc_score(y_val, y_proba))\n",
        "    metrics['specificity'].append(TN / (TN + FP))\n",
        "\n",
        "    print(f\"✅ Fold {fold_num} completed.\")\n",
        "    fold_num += 1\n",
        "\n",
        "# === (d) Display CV metrics ===\n",
        "print(\"\\n=== 📊 5-Fold CV Metrics (Default Cutoff = 0.5) ===\")\n",
        "for key in metrics:\n",
        "    mean = np.mean(metrics[key])\n",
        "    std = np.std(metrics[key])\n",
        "    print(f\"{key.capitalize():<12}: {mean:.4f} ± {std:.4f}\")\n",
        "\n",
        "# === (e) Final aggregated performance ===\n",
        "final_cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "TN, FP, FN, TP = final_cm.ravel()\n",
        "print(\"\\n=== 🔎 Final Aggregated Performance on All Out-of-Fold Predictions ===\")\n",
        "print(\"Confusion Matrix:\\n\", final_cm)\n",
        "print(f\"Specificity: {TN / (TN + FP):.4f}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5. Neural network with imbalance control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    mean_squared_error, brier_score_loss\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix XGBoost training issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- Extra cleaning before Neural network**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (j) Normalize features ===\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[e] --- --- --- Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# === (a) Compute Class Weights ===\n",
        "classes = np.unique(y_train)\n",
        "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "class_weights = dict(zip(classes, weights))\n",
        "print(\"✅ Computed class weights:\", class_weights)\n",
        "\n",
        "# === (b) Define Neural Network ===\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# === (c) Compile Model ===\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# === (d) Fit Model with Early Stopping and Class Weights ===\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (l) Predict ===\n",
        "\n",
        "# ✅ Step 1: Identify valid test indices (non-missing labels)\n",
        "valid_idx = ~y_test.isna()\n",
        "\n",
        "# ✅ Step 2: Filter out missing test labels\n",
        "y_test_clean = y_test[valid_idx].astype(int)\n",
        "X_test_clean = X_test_scaled[valid_idx]\n",
        "\n",
        "# ✅ Step 3: Predict probabilities and class labels\n",
        "y_proba = model.predict(X_test_clean, verbose=0).flatten()  # Get predicted probabilities\n",
        "y_pred = (y_proba > 0.5).astype(int)                         # Default threshold: 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 1: Diagnose test set\n",
        "print(\"🔍 Total length of y_test:\", len(y_test))\n",
        "print(\"❌ Missing values in y_test:\", y_test.isna().sum())\n",
        "\n",
        "# Optional: show problematic test rows (i.e., where no label is available)\n",
        "missing_rows = X_test[y_test.isna()]\n",
        "display(missing_rows.head())\n",
        "\n",
        "print(\"🧯 Missing y_test entries:\")\n",
        "print(y_test[y_test.isna()])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Step 2: Filter out NaN labels from y_test only\n",
        "valid_idx = ~y_test.isna()\n",
        "\n",
        "y_test_clean = y_test[valid_idx].astype(int)  # Ensure binary labels for evaluation\n",
        "y_pred_clean = y_pred             # Already of length 200\n",
        "y_proba_clean = y_proba           # Already of length 200\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (m) Evaluate Neural Network Model ===\n",
        "print(f\"✅ Accuracy:     {accuracy_score(y_test_clean, y_pred):.3f}\")\n",
        "print(f\"✅ AUC:          {roc_auc_score(y_test_clean, y_proba):.3f}\")\n",
        "print(f\"✅ Brier Score:  {brier_score_loss(y_test_clean, y_proba):.4f}\")\n",
        "print(f\"✅ MSE:          {mean_squared_error(y_test_clean, y_proba):.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test_clean, y_pred)\n",
        "print(\"✅ Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Optional: Unpack and display confusion matrix in labeled form\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "print(f\"   TN: {TN}, FP: {FP}, FN: {FN}, TP: {TP}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (ii) Enhanced Confusion Matrix (Stata-style) ===\n",
        "cm = confusion_matrix(y_test_clean, y_pred_clean, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"policy_action |   predicted\")\n",
        "print(\"              |     0        1     |   Total\")\n",
        "print(\"--------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"              | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"              | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"--------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total   | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"              | {col_perc_str} |   100.00\")\n",
        "print(\"              |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (iii) Scalar-style metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (n) Save Results ===\n",
        "test_clean = test.loc[valid_idx].copy()\n",
        "test_clean[\"nn_pred\"] = y_pred\n",
        "test_clean[\"nn_prob\"] = y_proba\n",
        "test_clean.to_csv(\"imbalneuralnet_predictions_policyaction.csv\", index=False)\n",
        "print(\"📁 Predictions saved to imbalneuralnet_predictions_policyaction.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Python Code for ROC Threshold Evaluation and Export**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === (1) Load final predictions ===\n",
        "df = pd.read_csv(\"imbalneuralnet_predictions_policyaction.csv\")\n",
        "\n",
        "# === (2) Define column names\n",
        "score_col = \"nn_prob\"         # Probabilities from XGBoost\n",
        "actual_col = \"policy_action\"   # True label from gold test set\n",
        "\n",
        "# === (3) Filter clean data ===\n",
        "df = df[[score_col, actual_col]].dropna()\n",
        "df['score_round'] = df[score_col].round(4)  # Round for consistency\n",
        "\n",
        "# === (4) Prepare ROC table ===\n",
        "thresholds = np.unique(df['score_round'])\n",
        "results = []\n",
        "\n",
        "P = (df[actual_col] == 1).sum()\n",
        "N = (df[actual_col] == 0).sum()\n",
        "\n",
        "for thresh in thresholds:\n",
        "    pred = (df[score_col] > thresh).astype(int)\n",
        "    TP = ((df[actual_col] == 1) & (pred == 1)).sum()\n",
        "    FP = ((df[actual_col] == 0) & (pred == 1)).sum()\n",
        "    FN = ((df[actual_col] == 1) & (pred == 0)).sum()\n",
        "    TN = ((df[actual_col] == 0) & (pred == 0)).sum()\n",
        "\n",
        "    TPR = TP / P if P > 0 else 0\n",
        "    FPR = FP / N if N > 0 else 0\n",
        "    distance = np.sqrt((1 - TPR) ** 2 + (FPR) ** 2)\n",
        "\n",
        "    results.append({\n",
        "        \"cutoff\": thresh,\n",
        "        \"TPR\": TPR,\n",
        "        \"FPR\": FPR,\n",
        "        \"distance\": distance,\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"TN\": TN,\n",
        "        \"FN\": FN\n",
        "    })\n",
        "\n",
        "# === (5) Convert to DataFrame ===\n",
        "roc_df = pd.DataFrame(results)\n",
        "\n",
        "# === (6) Find best threshold ===\n",
        "best_row = roc_df.loc[roc_df['distance'].idxmin()]\n",
        "best_cutoff = best_row['cutoff']\n",
        "\n",
        "print(\"\\n🎯 Optimal Threshold Results\")\n",
        "print(f\"Best cutoff       : {best_cutoff:.4f}\")\n",
        "print(f\"True Positive Rate: {best_row['TPR']:.4f}\")\n",
        "print(f\"False Positive Rate: {best_row['FPR']:.4f}\")\n",
        "print(f\"Min Distance      : {best_row['distance']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (7) Save all ROC points to CSV for review ===\n",
        "roc_df.to_csv(\"rocpoints_imbalneuralnet.csv\", index=False)\n",
        "print(\"📁 All thresholds and ROC stats saved to: rocpoints_imbalneuralnet.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# === (1) Apply Best Cutoff ===\n",
        "best_cutoff =  0.3612  # Replace with actual best threshold\n",
        "df[\"final_pred\"] = (df[\"nn_prob\"] > best_cutoff).astype(int)\n",
        "\n",
        "# Extract clean y_true and y_pred (already NaNs dropped earlier)\n",
        "y_true = df[\"policy_action\"]\n",
        "y_pred = df[\"final_pred\"]\n",
        "\n",
        "# === (2) Compute Confusion Matrix ===\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "row_perc = cm / cm.sum(axis=1, keepdims=True) * 100\n",
        "col_perc = cm / cm.sum(axis=0, keepdims=True) * 100\n",
        "col_totals = cm.sum(axis=0)\n",
        "grand_total = cm.sum()\n",
        "\n",
        "# === (3) Stata-style Table Output ===\n",
        "print(\"\\n+-------------------+\")\n",
        "print(\"| Key               |\")\n",
        "print(\"|-------------------|\")\n",
        "print(\"|     frequency     |\")\n",
        "print(\"|  row percentage   |\")\n",
        "print(\"| column percentage |\")\n",
        "print(\"+-------------------+\\n\")\n",
        "\n",
        "print(\"policy_action |   predicted\")\n",
        "print(\"             |     0        1     |   Total\")\n",
        "print(\"---------------+--------------------+----------\")\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "for i, label in enumerate(labels):\n",
        "    row = cm[i]\n",
        "    row_pct = row_perc[i]\n",
        "    col_pct = col_perc[i]\n",
        "    row_total = row.sum()\n",
        "    print(f\"         {label} | {row[0]:10,} {row[1]:10,} | {row_total:8,}\")\n",
        "    print(f\"           | {row_pct[0]:10.2f} {row_pct[1]:10.2f} |   100.00\")\n",
        "    print(f\"           | {col_pct[0]:10.2f} {col_pct[1]:10.2f} |\")\n",
        "    print(\"---------------+--------------------+----------\")\n",
        "\n",
        "col_total_str = \" \".join([f\"{val:10,}\" for val in col_totals])\n",
        "col_perc_str = \" \".join([f\"{(val/grand_total*100):10.2f}\" for val in col_totals])\n",
        "print(f\"     Total | {col_total_str} | {grand_total:8,}\")\n",
        "print(f\"           | {col_perc_str} |   100.00\")\n",
        "print(\"           |    100.00     100.00 |   100.00\")\n",
        "\n",
        "# === (4) Scalar Metrics ===\n",
        "Accuracy    = (TP + TN) / grand_total\n",
        "Precision   = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "Recall      = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "F1          = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) else 0.0\n",
        "Specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
        "\n",
        "print(\"\\n📐 Scalar-style Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {Accuracy:.8f}\")\n",
        "print(f\"Precision:       {Precision:.8f}\")\n",
        "print(f\"Recall:          {Recall:.8f}\")\n",
        "print(f\"F1 Score:        {F1:.8f}\")\n",
        "print(f\"Specificity:     {Specificity:.8f}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6. Neural network with imbalance control and cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, confusion_matrix,\n",
        "    mean_squared_error, brier_score_loss\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix XGBoost training issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- Extra cleaning before Neural network**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (j) Normalize features ===\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[e] --- --- --- Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# === (a) Cross-validation setup ===\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "metrics = {\n",
        "    'accuracy': [], 'precision': [], 'recall': [],\n",
        "    'f1': [], 'auc': [], 'specificity': []\n",
        "}\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "# === (b) CV loop ===\n",
        "fold_num = 1\n",
        "for train_idx, val_idx in cv.split(X_train_scaled, y_train):\n",
        "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    # Compute class weights for this fold\n",
        "    classes = np.unique(y_tr)\n",
        "    weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
        "    class_weights = dict(zip(classes, weights))\n",
        "\n",
        "    # Build model\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(X_tr.shape[1],)),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Train\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=64,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    y_proba = model.predict(X_val).flatten()\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "    # Save results\n",
        "    all_y_true.extend(y_val)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "    metrics['accuracy'].append(accuracy_score(y_val, y_pred))\n",
        "    metrics['precision'].append(precision_score(y_val, y_pred, zero_division=0))\n",
        "    metrics['recall'].append(recall_score(y_val, y_pred))\n",
        "    metrics['f1'].append(f1_score(y_val, y_pred))\n",
        "    metrics['auc'].append(roc_auc_score(y_val, y_proba))\n",
        "    metrics['specificity'].append(TN / (TN + FP))\n",
        "\n",
        "    print(f\"✅ Fold {fold_num} completed.\")\n",
        "    fold_num += 1\n",
        "\n",
        "# === (c) Report average ± std\n",
        "print(\"\\n=== 📊 5-Fold CV Metrics (Neural Net, Default Cutoff = 0.5) ===\")\n",
        "for key in metrics:\n",
        "    mean = np.mean(metrics[key])\n",
        "    std = np.std(metrics[key])\n",
        "    print(f\"{key.capitalize():<12}: {mean:.4f} ± {std:.4f}\")\n",
        "\n",
        "# === (d) Final classification report\n",
        "print(\"\\n=== 🔎 Final Aggregated Evaluation ===\")\n",
        "print(confusion_matrix(all_y_true, all_y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_y_true, all_y_pred, digits=4))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [8]. [Policy Action] Employ Imbalance Weight Xgboost (for final sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (a) Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, brier_score_loss\n",
        "\n",
        "\n",
        "# === (b) Load Data ===\n",
        "train = pd.read_csv(\"weak_labeled_policy_action_dataset_strict2.csv\", encoding=\"ISO-8859-1\")\n",
        "test = pd.read_csv(\"classified_climate_policy_actions.csv\", encoding=\"ISO-8859-1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[a] --- --- --- Dealing the time_event (single time: event_year_first and event_year_last)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse_year(text):\n",
        "    if pd.isna(text):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return parser.parse(str(text), dayfirst=False).year\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(str(text), dayfirst=True).year\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "train[\"event_year_first\"] = train[\"First event in timeline\"].apply(safe_parse_year)\n",
        "train[\"event_year_last\"] = train[\"Last event in timeline\"].apply(safe_parse_year)\n",
        "\n",
        "test[\"event_year_first\"] = test[\"First event in timeline\"].apply(safe_parse_year)\n",
        "test[\"event_year_last\"] = test[\"Last event in timeline\"].apply(safe_parse_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Check types and summary stats ===\n",
        "print(\"🔍 Column types:\\n\", train[[\"event_year_first\", \"event_year_last\"]].dtypes)\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for train:\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "print(\"\\n📊 Descriptive stats for test:\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].describe())\n",
        "\n",
        "# === Check value counts (frequency) ===\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_first':\")\n",
        "print(train[\"event_year_first\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "print(\"\\n🗓 Top 10 most frequent 'event_year_last':\")\n",
        "print(train[\"event_year_last\"].value_counts(dropna=False).sort_index().tail(10))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in training set\n",
        "print(\"📄 First 20 rows (Training Data):\")\n",
        "print(train[[\"event_year_first\", \"event_year_last\"]].head(20))\n",
        "\n",
        "# Show first 20 rows for event_year_first and event_year_last in testing set\n",
        "print(\"\\n📄 First 20 rows (Testing Data):\")\n",
        "print(test[[\"event_year_first\", \"event_year_last\"]].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[b] --- --- --- Dealing with multiple time event within 1 variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil import parser\n",
        "\n",
        "def safe_parse(date_str):\n",
        "    try:\n",
        "        return parser.parse(date_str, dayfirst=False)\n",
        "    except:\n",
        "        try:\n",
        "            return parser.parse(date_str, dayfirst=True)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_sorted_events(row):\n",
        "    types = row.get(\"Full timeline of events (types)\")\n",
        "    dates = row.get(\"Full timeline of events (dates)\")\n",
        "    \n",
        "    if pd.isna(types) or pd.isna(dates):\n",
        "        return None\n",
        "    \n",
        "    type_list = [t.strip() for t in str(types).split(\";\")]\n",
        "    date_list = [d.strip() for d in str(dates).split(\";\")]\n",
        "    \n",
        "    if len(type_list) != len(date_list):\n",
        "        return None  # mismatched length\n",
        "    \n",
        "    paired = [(t, safe_parse(d)) for t, d in zip(type_list, date_list)]\n",
        "    paired = [x for x in paired if x[1] is not None]  # drop invalid\n",
        "    return sorted(paired, key=lambda x: x[1])\n",
        "\n",
        "train[\"event_timeline\"] = train.apply(extract_sorted_events, axis=1)\n",
        "test[\"event_timeline\"] = test.apply(extract_sorted_events, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def order_after(df, move_col, after_col):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(move_col)\n",
        "    insert_at = cols.index(after_col) + 1\n",
        "    cols.insert(insert_at, move_col)\n",
        "    return df[cols]\n",
        "\n",
        "train = order_after(train, \"event_timeline\", \"Full timeline of events (dates)\")\n",
        "test = order_after(test, \"event_timeline\", \"Full timeline of events (dates)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train, test]:\n",
        "    # 1. Count of amendments\n",
        "    df[\"num_amendments\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: sum(1 for t, _ in x if t == \"Amended\") if x else 0\n",
        "    )\n",
        "\n",
        "    # 2. First \"Passed/Approved\" date\n",
        "    df[\"first_passed_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: next((d for t, d in x if t == \"Passed/Approved\"), None) if x else None\n",
        "    )\n",
        "\n",
        "    # 3. Last event date (regardless of type)\n",
        "    df[\"last_event_date\"] = df[\"event_timeline\"].apply(\n",
        "        lambda x: x[-1][1] if x else None\n",
        "    )\n",
        "\n",
        "    # 4. Duration in years\n",
        "    df[\"policy_duration_years\"] = df.apply(\n",
        "        lambda row: (row[\"last_event_date\"] - row[\"first_passed_date\"]).days / 365.25\n",
        "        if row[\"last_event_date\"] and row[\"first_passed_date\"] else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "print(train[[\"first_passed_date\", \"last_event_date\", \"num_amendments\", \"policy_duration_years\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to datetime if not already\n",
        "for df in [train, test]:\n",
        "    df[\"first_passed_date\"] = pd.to_datetime(df[\"first_passed_date\"], errors=\"coerce\")\n",
        "    df[\"last_event_date\"] = pd.to_datetime(df[\"last_event_date\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract year after conversion\n",
        "    df[\"event_year_passed\"] = df[\"first_passed_date\"].dt.year\n",
        "    df[\"event_year_last_event\"] = df[\"last_event_date\"].dt.year\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"🔍 Sample extracted years:\")\n",
        "print(train[[\"first_passed_date\", \"event_year_passed\", \"last_event_date\", \"event_year_last_event\"]].dropna().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[c] --- --- --- Extra cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (d) Define target and features ===\n",
        "target = \"policy_action\"\n",
        "exclude_cols = [\n",
        "    \"Document ID\", \"Document Title\", \"Family ID\", \"Family Title\",\n",
        "    \"Full timeline of events (types)\", target\n",
        "]\n",
        "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[target]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_informative_cols = []\n",
        "\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == \"object\":\n",
        "        # Skip if any row contains lists (unhashable)\n",
        "        if X_train[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            continue\n",
        "        # Drop if it's long text or URL\n",
        "        if X_train[col].astype(str).str.len().mean() > 100 or \\\n",
        "           X_train[col].astype(str).str.contains(\"http|www|pdf|doc\", case=False).mean() > 0.3:\n",
        "            non_informative_cols.append(col)\n",
        "        # Drop if almost every value is unique (like IDs)\n",
        "        elif X_train[col].nunique(dropna=True) > 0.9 * len(X_train):\n",
        "            non_informative_cols.append(col)\n",
        "\n",
        "print(\"🔎 Non-informative columns to drop:\\n\", non_informative_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column\tWhy Drop?\n",
        "# Family Summary\tLong free-text with highly variable length — better for NLP than tabular\n",
        "# Collection Description(s)\tOften redundant and descriptive, not structured\n",
        "# Document Content URL\tJust a URL — no predictive value\n",
        "# Internal Document ID\tUnique ID — not informative, high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Drop non-use variables from both train and test ===\n",
        "X_train.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n",
        "X_test.drop(columns=non_informative_cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (e) Convert object columns to numeric where possible ===\n",
        "for df in [X_train, X_test]:\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === (f) One-hot encode selected categorical variables ===\n",
        "categorical_vars = [\n",
        " \"Document Type\", \"Topic/Response\", \"Sector\", \"Instrument\",\n",
        "    \"Category\", \"Framework\", \"Hazard\", \"Author\", \"Author Type\",\n",
        "    \"Geographies\", \"Source\", \"Geography ISOs\"  # <-- New additions\n",
        "]\n",
        "categorical_vars = [col for col in categorical_vars if col in X_train.columns]\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_vars, dummy_na=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_vars, dummy_na=True)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check remaining numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"🔢 Total numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "# Step 2: Count missing values per numeric column\n",
        "missing_train = X_train[numeric_cols].isna().sum()\n",
        "missing_train = missing_train[missing_train > 0]\n",
        "\n",
        "print(\"\\n📌 Numeric columns with missing values in training set:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Step 3: Optional – show percentage missing\n",
        "percent_missing = (missing_train / len(X_train) * 100).round(2)\n",
        "print(\"\\n📊 Percentage missing per column:\")\n",
        "print(percent_missing.sort_values(ascending=False))\n",
        "\n",
        "# Step 4: Show sample rows with missing values in top offender column\n",
        "if not missing_train.empty:\n",
        "    top_col = missing_train.idxmax()\n",
        "    print(f\"\\n🔎 Sample rows with missing in '{top_col}':\")\n",
        "    display(X_train[X_train[top_col].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Summary of Missingness\n",
        "\n",
        "| Column                          | Missing (%) | Reason or Type              | Action                                     |\n",
        "|----------------------------------|--------------|------------------------------|---------------------------------------------|\n",
        "| `Collection Title(s)`            | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Collection Description(s)`      | 100%         | Metadata, unused             | Drop                                     |\n",
        "| `Document Variant`               | 100%         | Metadata                     | Drop                                     |\n",
        "| `Language`                       | 100%         | Non-informative              | Drop                                     |\n",
        "| `First event in timeline`        | 100%         | Raw date field               | Drop (already extracted year)            |\n",
        "| `Last event in timeline`         | 100%         | Raw date field               | Drop                                     |\n",
        "| `Full timeline of events (dates)`| 100%         | Raw date field               | Drop                                     |\n",
        "| `event_timeline`                 | 100%         | List of tuples               | Drop for ML; useful for NLP only         |\n",
        "| `Date Added to System`           | 100%         | System metadata              | Drop                                     |\n",
        "| `Last Modified on System`        | 100%         | System metadata              | Drop                                     |\n",
        "| `Internal Family ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Corpus ID`             | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Internal Collection ID(s)`      | 100%         | Internal ID                  | Drop                                     |\n",
        "| `Document Role`                  | 100%         | Possibly encoded elsewhere   | Drop                                     |\n",
        "| `Keyword`                        | 100%         | Free text                    | Drop                                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'Collection Title(s)', 'Collection Description(s)', 'Document Variant', 'Language',\n",
        "    'First event in timeline', 'Last event in timeline', 'Full timeline of events (dates)',\n",
        "    'event_timeline', 'Date Added to System', 'Last Modified on System',\n",
        "    'Internal Family ID', 'Internal Corpus ID', 'Internal Collection ID(s)',\n",
        "    'Document Role', 'Keyword'\n",
        "]\n",
        "\n",
        "X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns], inplace=True)\n",
        "X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns], inplace=True)\n",
        "print(\"✅ Dropped 100% missing / non-informative columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (g) Impute missing values safely ===\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop numeric columns with all NaNs in training set\n",
        "all_nan_cols = [col for col in numeric_cols if X_train[col].isna().all()]\n",
        "X_train.drop(columns=all_nan_cols, inplace=True)\n",
        "X_test.drop(columns=all_nan_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# Re-identify numeric columns after dropping\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute if we still have numeric columns\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
        "else:\n",
        "    print(\"⚠️ No numeric columns left for imputation.\")\n",
        "\n",
        "\n",
        "# ✅ Confirm imputation worked\n",
        "print(\"✅ Remaining numeric columns:\", len(numeric_cols))\n",
        "print(\"🔍 Any missing values left in training set?\", X_train[numeric_cols].isna().any().any())\n",
        "print(\"🔍 Any missing values left in test set?\", X_test[numeric_cols].isna().any().any())\n",
        "\n",
        "# Optional: Preview a few numeric columns\n",
        "X_train[numeric_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop datetime columns that is not useful\n",
        "X_train.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "X_test.drop(columns=[\"first_passed_date\", \"last_event_date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "print(\"✅ Dropped datetime columns to fix XGBoost training issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[d] --- --- --- model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# === (h) Compute imbalance weight ===\n",
        "n_pos = (y_train == 1).sum()\n",
        "n_neg = (y_train == 0).sum()\n",
        "scale_pos_weight = n_neg / n_pos\n",
        "print(f\"✅ Calculated scale_pos_weight: {scale_pos_weight:.4f}\")\n",
        "\n",
        "# === (i) Train Imbalanced XGBoost Model ===\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.2,\n",
        "    n_jobs=-1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    scale_pos_weight=scale_pos_weight\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"✅ Weighted XGBoost model trained.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Apply Model to Full Sample\n",
        "X_full = pd.concat([X_train, X_test], ignore_index=True)\n",
        "\n",
        "# === Predict on full sample ===\n",
        "xgb_prob = model.predict_proba(X_full)[:, 1]\n",
        "xgb_pred = (xgb_prob > 0.3734).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Attach predictions back to original data ===\n",
        "df_full = pd.concat([train, test], ignore_index=True).copy()\n",
        "df_full[\"xgb_prob\"] = xgb_prob\n",
        "df_full[\"xgb_pred\"] = xgb_pred\n",
        "\n",
        "# === Save final predictions ===\n",
        "df_full.to_csv(\"final_climate_policy_predictions_xgboost.csv\", index=False)\n",
        "print(\"📁 Final predictions saved to: final_climate_policy_predictions_xgboost.csv\")\n",
        "print(\"✅ Includes: xgb_prob (probability), xgb_pred (final 1/0 label)\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[e] --- --- --- Aggregate to country-year data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === (a) Load Final Prediction Data ===\n",
        "file_path = \"final_climate_policy_predictions_xgboost.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# === (b) Step 1: Extract year from \"First event in timeline\"\n",
        "data[\"document_year\"] = pd.to_datetime(data[\"First event in timeline\"], errors='coerce').dt.year\n",
        "\n",
        "# === (c) Step 2: Drop rows with missing or invalid year or country\n",
        "data = data.dropna(subset=[\"document_year\", \"Geographies\"])\n",
        "data[\"document_year\"] = data[\"document_year\"].astype(int)\n",
        "\n",
        "# === (d) Aggregation to Country-Year Panel ===\n",
        "policy_panel = (\n",
        "    data\n",
        "    .groupby([\"Geographies\", \"document_year\"])\n",
        "    .agg(\n",
        "        num_docs=(\"Document ID\", \"count\"),\n",
        "        avg_xgb_prob=(\"xgb_prob\", \"mean\"),\n",
        "        share_xgb_pred_1=(\"xgb_pred\", \"mean\"),\n",
        "        sum_xgb_pred_1=(\"xgb_pred\", \"sum\")\n",
        "    )\n",
        "    .reset_index()\n",
        "    .sort_values([\"Geographies\", \"document_year\"])\n",
        ")\n",
        "\n",
        "# === (e) Round for clarity\n",
        "policy_panel[[\"avg_xgb_prob\", \"share_xgb_pred_1\"]] = \\\n",
        "    policy_panel[[\"avg_xgb_prob\", \"share_xgb_pred_1\"]].round(3)\n",
        "\n",
        "# === (f) Preview and Save\n",
        "print(policy_panel.head(10))\n",
        "policy_panel.to_csv(\"aggregated_policyaction_panel.csv\", index=False)\n",
        "print(\"✅ Aggregated country-year panel saved to: aggregated_policyaction_panel.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### refine version include iso code\n",
        "import pandas as pd\n",
        "\n",
        "# === (a) Load Final Prediction Data ===\n",
        "file_path = \"final_climate_policy_predictions_xgboost.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# === (b) Extract document year from \"First event in timeline\"\n",
        "data[\"document_year\"] = pd.to_datetime(data[\"First event in timeline\"], errors='coerce').dt.year\n",
        "\n",
        "# === (c) Drop rows with missing year or ISO code\n",
        "data = data.dropna(subset=[\"document_year\", \"Geography ISOs\"])\n",
        "data[\"document_year\"] = data[\"document_year\"].astype(int)\n",
        "\n",
        "# === (d) Group and Aggregate to ISO3-Year Level ===\n",
        "panel = (\n",
        "    data\n",
        "    .groupby([\"Geography ISOs\", \"document_year\"])\n",
        "    .agg(\n",
        "        num_docs=(\"Document ID\", \"count\"),\n",
        "        avg_xgb_prob=(\"xgb_prob\", \"mean\"),\n",
        "        share_xgb_pred_1=(\"xgb_pred\", \"mean\"),\n",
        "        sum_xgb_pred_1=(\"xgb_pred\", \"sum\")\n",
        "    )\n",
        "    .reset_index()\n",
        "    .sort_values([\"Geography ISOs\", \"document_year\"])\n",
        ")\n",
        "\n",
        "# === (e) Rename columns\n",
        "panel.rename(columns={\n",
        "    \"Geography ISOs\": \"iso3\",\n",
        "    \"document_year\": \"year\"\n",
        "}, inplace=True)\n",
        "\n",
        "# === (f) Round selected columns\n",
        "panel[[\"avg_xgb_prob\", \"share_xgb_pred_1\"]] = panel[[\"avg_xgb_prob\", \"share_xgb_pred_1\"]].round(3)\n",
        "\n",
        "# === (g) Save to file\n",
        "panel.to_csv(\"aggregated_policyaction_panel_iso3.csv\", index=False)\n",
        "print(\"✅ Saved panel to: aggregated_policyaction_panel_iso3.csv\")\n",
        "\n",
        "# === (h) Preview\n",
        "print(panel.head(10))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (BERTopic)",
      "language": "python",
      "name": "bertopic_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
